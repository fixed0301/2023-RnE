{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:22:40.039969800Z",
     "start_time": "2023-12-17T11:22:40.024349500Z"
    }
   },
   "id": "91a24a48c60e7953"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "CFG = {'batch_size': 32,\n",
    "       'learning_rate': 1e-4,\n",
    "       'seed':24,\n",
    "       'epochs': 100   \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:22:50.057591500Z",
     "start_time": "2023-12-17T11:22:50.041965500Z"
    }
   },
   "id": "14d2065c8d4dc0c6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 불가능 상태\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "       device = 'cuda:0'\n",
    "       print('현재 가상환경 GPU 사용 가능 상태')\n",
    "else:\n",
    "       device = 'cpu'\n",
    "       print('GPU 사용 불가능 상태')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:22:54.562585800Z",
     "start_time": "2023-12-17T11:22:54.531338300Z"
    }
   },
   "id": "2a6bb59c8c16479c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 24):\n",
    "       random.seed(seed)\n",
    "       np.random.seed(seed)\n",
    "       # torch.cuda.manual_seed(seed)\n",
    "       # torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:38.817767Z"
    }
   },
   "id": "18a26b6e0c9bac97"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tot_actions = 5\n",
    "actions_name = 'backward', 'sit', 'slide', 'swing', 'walk'\n",
    "min_data_len = 45"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:40.032073100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "actions_csv_dir = '../csv_1031/'\n",
    "dataset = []\n",
    "\n",
    "label_mapping = {'backward': 0,\n",
    "                 'sit': 1,\n",
    "                 'slide': 2,\n",
    "                 'swing': 3,\n",
    "                 'walk' : 4\n",
    "                 }\n",
    "\n",
    "def map_action_to_label(csv_name):\n",
    "       for action, label in label_mapping.items():\n",
    "              if action in csv_name.split('_')[0]:\n",
    "                     return label\n",
    "       return -1\n",
    "\n",
    "\n",
    "for action_csv in os.listdir(actions_csv_dir):\n",
    "       action_df = pd.read_csv(os.path.join(actions_csv_dir, action_csv))\n",
    "       \n",
    "       label = map_action_to_label(action_csv)\n",
    "       if label != -1:\n",
    "              for idx in range(0, len(action_df), int(min_data_len / 2)):\n",
    "                     seq_df = action_df[idx: idx + min_data_len] #길이만큼 데이터 자른 것(즉 length 만큼의 프레임)\n",
    "                     if len(seq_df) == min_data_len: # 딱 length에 개수 맞춰서 끊어서 넣으려고\n",
    "                            dataset.append({'key': label, 'value': seq_df}) # key에 slide, value에는 묶음 프레임 만큼이 담기겠네\n",
    "       #최종적으로 dataset에는 행동별로 dictionary 가 만들어져 들어간다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:40.871452900Z"
    }
   },
   "id": "20776e1e9089a511"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Nose_x', 'Nose_y', 'Nose_z', 'LEye_in_x', 'LEye_in_y', 'LEye_in_z',\n",
      "       'LEye_x', 'LEye_y', 'LEye_z', 'LEye_out_x', 'LEye_out_y', 'LEye_out_z',\n",
      "       'REye_in_x', 'REye_in_y', 'REye_in_z', 'REye_x', 'REye_y', 'REye_z',\n",
      "       'REye_out_x', 'REye_out_y', 'REye_out_z', 'LEar_x', 'LEar_y', 'LEar_z',\n",
      "       'REar_x', 'REar_y', 'REar_z', 'LMouth_x', 'LMouth_y', 'LMouth_z',\n",
      "       'RMouth_x', 'RMouth_y', 'RMouth_z', 'LShoulder_x', 'LShoulder_y',\n",
      "       'LShoulder_z', 'RShoulder_x', 'RShoulder_y', 'RShoulder_z', 'LElbow_x',\n",
      "       'LElbow_y', 'LElbow_z', 'RElbow_x', 'RElbow_y', 'RElbow_z', 'LWrist_x',\n",
      "       'LWrist_y', 'LWrist_z', 'RWrist_x', 'RWrist_y', 'RWrist_z', 'LPinky_x',\n",
      "       'LPinky_y', 'LPinky_z', 'RPinky_x', 'RPinky_y', 'RPinky_z', 'LIndex_x',\n",
      "       'LIndex_y', 'LIndex_z', 'RIndex_x', 'RIndex_y', 'RIndex_z', 'LThumb_x',\n",
      "       'LThumb_y', 'LThumb_z', 'RThumb_x', 'RThumb_y', 'RThumb_z', 'LHip_x',\n",
      "       'LHip_y', 'LHip_z', 'RHip_x', 'RHip_y', 'RHip_z', 'LKnee_x', 'LKnee_y',\n",
      "       'LKnee_z', 'RKnee_x', 'RKnee_y', 'RKnee_z', 'LAnkle_x', 'LAnkle_y',\n",
      "       'LAnkle_z', 'RAnkle_x', 'RAnkle_y', 'RAnkle_z', 'LHeel_x', 'LHeel_y',\n",
      "       'LHeel_z', 'RHeel_x', 'RHeel_y', 'RHeel_z', 'LFIndex_x', 'LFIndex_y',\n",
      "       'LFIndex_z', 'RFIndex_x', 'RFIndex_y', 'RFIndex_z'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['value'].columns) # z축 까지 99 (33 * 3)차원"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:44.006195700Z"
    }
   },
   "id": "d656657e15c7e45f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "       def __init__(self, dataset): #모든 행동을 통합한 df가 들어가야함\n",
    "              self.x = []\n",
    "              self.y = []\n",
    "              for dic in dataset:\n",
    "                     self.y.append(dic['key']) #key 값에는 actions 들어감\n",
    "                     self.x.append(dic['value']) #action마다의 data 들어감\n",
    "\n",
    "       def __getitem__(self, index): #index는 행동의 index\n",
    "              data = self.x[index] # x에는 꺼내 쓸 (행동마다 45개 묶음프레임)의 데이터\n",
    "              label = self.y[index]\n",
    "              return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "       def __len__(self):\n",
    "              return len(self.x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:44.864016Z"
    }
   },
   "id": "f6106f1224437afe"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n",
      "531, 66, 67\n"
     ]
    }
   ],
   "source": [
    "train_test_val_ratio = [0.8, 0.1, 0.1]\n",
    "print(len(dataset))\n",
    "train_len = int(len(dataset) * train_test_val_ratio[0])\n",
    "val_len = int(len(dataset) * train_test_val_ratio[1])\n",
    "test_len = len(dataset) - train_len - val_len\n",
    "print('{}, {}, {}'.format(train_len, val_len, test_len))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T10:41:46.040531100Z"
    }
   },
   "id": "ba330d25c6c6f524"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=CFG['batch_size'])\n",
    "val_loader = DataLoader(valid_data, batch_size=CFG['batch_size'])\n",
    "test_loader = DataLoader(test_data, batch_size=CFG['batch_size'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T10:41:52.604850200Z",
     "start_time": "2023-12-14T10:41:52.546038100Z"
    }
   },
   "id": "9fa7ab00e37041cc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model1, self).__init__()\n",
    "              self.lstm1 = nn.LSTM(input_size=99, hidden_size=128, num_layers=1, batch_first=True) #input은  45 * 3(x, y z)\n",
    "              self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n",
    "              self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)\n",
    "              self.dropout1 = nn.Dropout(0, 1)\n",
    "              self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
    "              self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n",
    "              self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "              self.dropout2 = nn.Dropout(0, 1)\n",
    "              self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)\n",
    "              self.fc = nn.Linear(32, 5) #분류할 클래스 5가지\n",
    "\n",
    "       def forward(self, x):\n",
    "              x, _ = self.lstm1(x)\n",
    "              x, _ = self.lstm2(x)\n",
    "              x, _ = self.lstm3(x)\n",
    "              x = self.dropout1(x)\n",
    "              x, _ = self.lstm4(x)\n",
    "              x, _ = self.lstm5(x)\n",
    "              x, _ = self.lstm6(x)\n",
    "              x = self.dropout2(x)\n",
    "              x, _ = self.lstm7(x)\n",
    "              x = self.fc(x[:, -1, :])\n",
    "              return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:08:03.964433900Z",
     "start_time": "2023-12-13T10:08:03.960426Z"
    }
   },
   "id": "d712906be6495443"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Model 2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "       def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "              super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "              assert hidden_channels % 2 == 0\n",
    "\n",
    "              self.input_channels = input_channels\n",
    "              self.hidden_channels = hidden_channels\n",
    "              self.kernel_size = kernel_size\n",
    "              self.num_features = 5\n",
    "\n",
    "              self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "              self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "              self.Wci = None\n",
    "              self.Wcf = None\n",
    "              self.Wco = None\n",
    "\n",
    "       def forward(self, x, h, c):\n",
    "              ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "              cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "              cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "              co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "              ch = co * torch.tanh(cc)\n",
    "              return ch, cc\n",
    "\n",
    "       def init_hidden(self, batch_size, hidden, shape):\n",
    "              if self.Wci is None:\n",
    "                     self.Wci = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "                     self.Wcf = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "                     self.Wco = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "              else:\n",
    "                     assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "                     assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "              return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda(),\n",
    "                      Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "       # input_channels corresponds to the first input feature map\n",
    "       # hidden state is a list of succeeding lstm layers.\n",
    "       def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "              super(ConvLSTM, self).__init__()\n",
    "              self.input_channels = [input_channels] + hidden_channels\n",
    "              self.hidden_channels = hidden_channels\n",
    "              self.kernel_size = kernel_size\n",
    "              self.num_layers = len(hidden_channels)\n",
    "              self.step = step\n",
    "              self.effective_step = effective_step\n",
    "              self._all_layers = []\n",
    "              for i in range(self.num_layers):\n",
    "                     name = 'cell{}'.format(i)\n",
    "                     cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "                     setattr(self, name, cell)\n",
    "                     self._all_layers.append(cell)\n",
    "\n",
    "       def forward(self, input):\n",
    "              internal_state = []\n",
    "              outputs = []\n",
    "              for step in range(self.step):\n",
    "                     x = input\n",
    "                     for i in range(self.num_layers):\n",
    "                            # all cells are initialized in the first step\n",
    "                            name = 'cell{}'.format(i)\n",
    "                            if step == 0:\n",
    "                                   bsize, _, height, width = x.size()\n",
    "                                   (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                                            shape=(height, width))\n",
    "                                   internal_state.append((h, c))\n",
    "\n",
    "                            # do forward\n",
    "                            (h, c) = internal_state[i]\n",
    "                            x, new_c = getattr(self, name)(x, h, c)\n",
    "                            internal_state[i] = (x, new_c)\n",
    "                     # only record effective steps\n",
    "                     if step in self.effective_step:\n",
    "                            outputs.append(x)\n",
    "\n",
    "              return outputs, (x, new_c)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T10:43:04.648171400Z",
     "start_time": "2023-12-14T10:43:04.585680900Z"
    }
   },
   "id": "a8e56a0b61128b5d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_1728\\1906980042.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# gradient check\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size=3, step=5,\n\u001B[0m\u001B[0;32m      3\u001B[0m                     effective_step=[4]).cuda()\n\u001B[0;32m      4\u001B[0m \u001B[0mloss_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMSELoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mcuda\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m    687\u001B[0m             \u001B[0mModule\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    688\u001B[0m         \"\"\"\n\u001B[1;32m--> 689\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    690\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    691\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mipu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    577\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    578\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 579\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    580\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    581\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    577\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    578\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 579\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    580\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    581\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    600\u001B[0m             \u001B[1;31m# `with torch.no_grad():`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    601\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 602\u001B[1;33m                 \u001B[0mparam_applied\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    603\u001B[0m             \u001B[0mshould_use_set_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    604\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mshould_use_set_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    687\u001B[0m             \u001B[0mModule\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    688\u001B[0m         \"\"\"\n\u001B[1;32m--> 689\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    690\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    691\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mipu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mUnion\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001B[0m in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    209\u001B[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001B[0;32m    210\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'_cuda_getDeviceCount'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mAssertionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Torch not compiled with CUDA enabled\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_cudart\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m             raise AssertionError(\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# gradient check\n",
    "convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size=3, step=5,\n",
    "                    effective_step=[4]).cuda()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "input = Variable(torch.randn(1, 512, 64, 32)).cuda()\n",
    "target = Variable(torch.randn(1, 32, 64, 32)).double().cuda()\n",
    "\n",
    "output = convlstm(input)\n",
    "output = output[0][0].double()\n",
    "res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T10:43:09.826997300Z",
     "start_time": "2023-12-14T10:43:07.372971800Z"
    }
   },
   "id": "2831846bdf0d3426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model 3\n",
    "class CGRU_cell(nn.Module):\n",
    "       \"\"\"\n",
    "       ConvGRU Cell\n",
    "       \"\"\"\n",
    "       def __init__(self, shape, input_channels, filter_size, num_features):\n",
    "              super(CGRU_cell, self).__init__()\n",
    "              self.shape = shape\n",
    "              self.input_channels = input_channels\n",
    "              # kernel_size of input_to_state equals state_to_state\n",
    "              self.filter_size = filter_size\n",
    "              self.num_features = num_features\n",
    "              self.padding = (filter_size - 1) // 2\n",
    "              self.conv1 = nn.Sequential(\n",
    "                     nn.Conv2d(self.input_channels + self.num_features,\n",
    "                               2 * self.num_features, self.filter_size, 1,\n",
    "                               self.padding),\n",
    "                     nn.GroupNorm(2 * self.num_features // 32, 2 * self.num_features))\n",
    "              self.conv2 = nn.Sequential(\n",
    "                     nn.Conv2d(self.input_channels + self.num_features,\n",
    "                               self.num_features, self.filter_size, 1, self.padding),\n",
    "                     nn.GroupNorm(self.num_features // 32, self.num_features))\n",
    "\n",
    "       def forward(self, inputs=None, hidden_state=None, seq_len=10):\n",
    "              # seq_len=10 for moving_mnist\n",
    "              if hidden_state is None:\n",
    "                     htprev = torch.zeros(inputs.size(1), self.num_features,\n",
    "                                          self.shape[0], self.shape[1]).cuda()\n",
    "              else:\n",
    "                     htprev = hidden_state\n",
    "              output_inner = []\n",
    "              for index in range(seq_len):\n",
    "                     if inputs is None:\n",
    "                            x = torch.zeros(htprev.size(0), self.input_channels,\n",
    "                                            self.shape[0], self.shape[1]).cuda()\n",
    "                     else:\n",
    "                            x = inputs[index, ...]\n",
    "\n",
    "                     combined_1 = torch.cat((x, htprev), 1)  # X_t + H_t-1\n",
    "                     gates = self.conv1(combined_1)  # W * (X_t + H_t-1)\n",
    "\n",
    "                     zgate, rgate = torch.split(gates, self.num_features, dim=1)\n",
    "                     # zgate, rgate = gates.chunk(2, 1)\n",
    "                     z = torch.sigmoid(zgate)\n",
    "                     r = torch.sigmoid(rgate)\n",
    "\n",
    "                     combined_2 = torch.cat((x, r * htprev),\n",
    "                                            1)  # h' = tanh(W*(x+r*H_t-1))\n",
    "                     ht = self.conv2(combined_2)\n",
    "                     ht = torch.tanh(ht)\n",
    "                     htnext = (1 - z) * htprev + z * ht\n",
    "                     output_inner.append(htnext)\n",
    "                     htprev = htnext\n",
    "              return torch.stack(output_inner), htnext\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a40e1fb638676a05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gradient check\n",
    "cgru = CGRU_cell((32, 45, ), input_channels=512 ) \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "input = Variable(torch.randn(1, 512, 64, 32)).cuda()\n",
    "target = Variable(torch.randn(1, 32, 64, 32)).double().cuda()\n",
    "\n",
    "output = convlstm(input)\n",
    "output = output[0][0].double()\n",
    "res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d10bb72ba73cd4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: -0.19235 valid loss:  -0.19264 lr: 0.00100 \n",
      "Validation loss decreased (inf --> -0.192640).  Saving model ...\n",
      "Validation loss decreased (-0.192640 --> -0.193880).  Saving model ...\n",
      "Validation loss decreased (-0.193880 --> -0.195230).  Saving model ...\n",
      "Validation loss decreased (-0.195230 --> -0.196600).  Saving model ...\n",
      "Validation loss decreased (-0.196600 --> -0.198010).  Saving model ...\n",
      "Validation loss decreased (-0.198010 --> -0.199440).  Saving model ...\n",
      "Validation loss decreased (-0.199440 --> -0.200890).  Saving model ...\n",
      "Validation loss decreased (-0.200890 --> -0.202360).  Saving model ...\n",
      "Validation loss decreased (-0.202360 --> -0.203870).  Saving model ...\n",
      "Validation loss decreased (-0.203870 --> -0.205400).  Saving model ...\n",
      "Epoch: 10, loss: -0.20832 valid loss:  -0.20695 lr: 0.00100 \n",
      "Validation loss decreased (-0.205400 --> -0.206950).  Saving model ...\n",
      "Validation loss decreased (-0.206950 --> -0.208540).  Saving model ...\n",
      "Validation loss decreased (-0.208540 --> -0.210160).  Saving model ...\n",
      "Validation loss decreased (-0.210160 --> -0.211800).  Saving model ...\n",
      "Validation loss decreased (-0.211800 --> -0.213480).  Saving model ...\n",
      "Validation loss decreased (-0.213480 --> -0.215200).  Saving model ...\n",
      "Validation loss decreased (-0.215200 --> -0.216940).  Saving model ...\n",
      "Validation loss decreased (-0.216940 --> -0.218730).  Saving model ...\n",
      "Validation loss decreased (-0.218730 --> -0.220540).  Saving model ...\n",
      "Validation loss decreased (-0.220540 --> -0.222400).  Saving model ...\n",
      "Epoch: 20, loss: -0.22852 valid loss:  -0.22430 lr: 0.00100 \n",
      "Validation loss decreased (-0.222400 --> -0.224300).  Saving model ...\n",
      "Validation loss decreased (-0.224300 --> -0.226240).  Saving model ...\n",
      "Validation loss decreased (-0.226240 --> -0.228220).  Saving model ...\n",
      "Validation loss decreased (-0.228220 --> -0.230240).  Saving model ...\n",
      "Validation loss decreased (-0.230240 --> -0.232310).  Saving model ...\n",
      "Validation loss decreased (-0.232310 --> -0.234420).  Saving model ...\n",
      "Validation loss decreased (-0.234420 --> -0.236590).  Saving model ...\n",
      "Validation loss decreased (-0.236590 --> -0.238800).  Saving model ...\n",
      "Validation loss decreased (-0.238800 --> -0.241060).  Saving model ...\n",
      "Validation loss decreased (-0.241060 --> -0.243380).  Saving model ...\n",
      "Epoch: 30, loss: -0.25418 valid loss:  -0.24575 lr: 0.00100 \n",
      "Validation loss decreased (-0.243380 --> -0.245750).  Saving model ...\n",
      "Validation loss decreased (-0.245750 --> -0.248170).  Saving model ...\n",
      "Validation loss decreased (-0.248170 --> -0.250660).  Saving model ...\n",
      "Validation loss decreased (-0.250660 --> -0.253200).  Saving model ...\n",
      "Validation loss decreased (-0.253200 --> -0.255800).  Saving model ...\n",
      "Validation loss decreased (-0.255800 --> -0.258460).  Saving model ...\n",
      "Validation loss decreased (-0.258460 --> -0.261190).  Saving model ...\n",
      "Validation loss decreased (-0.261190 --> -0.263970).  Saving model ...\n",
      "Validation loss decreased (-0.263970 --> -0.266820).  Saving model ...\n",
      "Validation loss decreased (-0.266820 --> -0.269740).  Saving model ...\n",
      "Epoch: 40, loss: -0.28728 valid loss:  -0.27272 lr: 0.00100 \n",
      "Validation loss decreased (-0.269740 --> -0.272720).  Saving model ...\n",
      "Validation loss decreased (-0.272720 --> -0.275760).  Saving model ...\n",
      "Validation loss decreased (-0.275760 --> -0.278870).  Saving model ...\n",
      "Validation loss decreased (-0.278870 --> -0.282050).  Saving model ...\n",
      "Validation loss decreased (-0.282050 --> -0.285290).  Saving model ...\n",
      "Validation loss decreased (-0.285290 --> -0.288580).  Saving model ...\n",
      "Validation loss decreased (-0.288580 --> -0.291940).  Saving model ...\n",
      "Validation loss decreased (-0.291940 --> -0.295360).  Saving model ...\n",
      "Validation loss decreased (-0.295360 --> -0.298840).  Saving model ...\n",
      "Validation loss decreased (-0.298840 --> -0.302360).  Saving model ...\n",
      "Epoch: 50, loss: -0.32906 valid loss:  -0.30593 lr: 0.00100 \n",
      "Validation loss decreased (-0.302360 --> -0.305930).  Saving model ...\n",
      "Validation loss decreased (-0.305930 --> -0.309550).  Saving model ...\n",
      "Validation loss decreased (-0.309550 --> -0.313210).  Saving model ...\n",
      "Validation loss decreased (-0.313210 --> -0.316900).  Saving model ...\n",
      "Validation loss decreased (-0.316900 --> -0.320610).  Saving model ...\n",
      "Validation loss decreased (-0.320610 --> -0.324350).  Saving model ...\n",
      "Validation loss decreased (-0.324350 --> -0.328100).  Saving model ...\n",
      "Validation loss decreased (-0.328100 --> -0.331850).  Saving model ...\n",
      "Validation loss decreased (-0.331850 --> -0.335600).  Saving model ...\n",
      "Validation loss decreased (-0.335600 --> -0.339340).  Saving model ...\n",
      "Epoch: 60, loss: -0.37693 valid loss:  -0.34307 lr: 0.00100 \n",
      "Validation loss decreased (-0.339340 --> -0.343070).  Saving model ...\n",
      "Validation loss decreased (-0.343070 --> -0.346760).  Saving model ...\n",
      "Validation loss decreased (-0.346760 --> -0.350420).  Saving model ...\n",
      "Validation loss decreased (-0.350420 --> -0.354040).  Saving model ...\n",
      "Validation loss decreased (-0.354040 --> -0.357600).  Saving model ...\n",
      "Validation loss decreased (-0.357600 --> -0.361110).  Saving model ...\n",
      "Validation loss decreased (-0.361110 --> -0.364540).  Saving model ...\n",
      "Validation loss decreased (-0.364540 --> -0.367910).  Saving model ...\n",
      "Validation loss decreased (-0.367910 --> -0.371200).  Saving model ...\n",
      "Validation loss decreased (-0.371200 --> -0.374400).  Saving model ...\n",
      "Epoch: 70, loss: -0.42244 valid loss:  -0.37751 lr: 0.00100 \n",
      "Validation loss decreased (-0.374400 --> -0.377510).  Saving model ...\n",
      "Validation loss decreased (-0.377510 --> -0.380540).  Saving model ...\n",
      "Validation loss decreased (-0.380540 --> -0.383470).  Saving model ...\n",
      "Validation loss decreased (-0.383470 --> -0.386300).  Saving model ...\n",
      "Validation loss decreased (-0.386300 --> -0.389030).  Saving model ...\n",
      "Validation loss decreased (-0.389030 --> -0.391670).  Saving model ...\n",
      "Validation loss decreased (-0.391670 --> -0.394200).  Saving model ...\n",
      "Validation loss decreased (-0.394200 --> -0.396640).  Saving model ...\n",
      "Validation loss decreased (-0.396640 --> -0.398980).  Saving model ...\n",
      "Validation loss decreased (-0.398980 --> -0.401230).  Saving model ...\n",
      "Epoch: 80, loss: -0.45726 valid loss:  -0.40338 lr: 0.00100 \n",
      "Validation loss decreased (-0.401230 --> -0.403380).  Saving model ...\n",
      "Validation loss decreased (-0.403380 --> -0.405430).  Saving model ...\n",
      "Validation loss decreased (-0.405430 --> -0.407400).  Saving model ...\n",
      "Validation loss decreased (-0.407400 --> -0.409280).  Saving model ...\n",
      "Validation loss decreased (-0.409280 --> -0.411080).  Saving model ...\n",
      "Validation loss decreased (-0.411080 --> -0.412800).  Saving model ...\n",
      "Validation loss decreased (-0.412800 --> -0.414430).  Saving model ...\n",
      "Validation loss decreased (-0.414430 --> -0.416000).  Saving model ...\n",
      "Validation loss decreased (-0.416000 --> -0.417490).  Saving model ...\n",
      "Validation loss decreased (-0.417490 --> -0.418910).  Saving model ...\n",
      "Epoch: 90, loss: -0.48025 valid loss:  -0.42027 lr: 0.00100 \n",
      "Validation loss decreased (-0.418910 --> -0.420270).  Saving model ...\n",
      "Validation loss decreased (-0.420270 --> -0.421570).  Saving model ...\n",
      "Validation loss decreased (-0.421570 --> -0.422800).  Saving model ...\n",
      "Validation loss decreased (-0.422800 --> -0.423980).  Saving model ...\n",
      "Validation loss decreased (-0.423980 --> -0.425100).  Saving model ...\n",
      "Validation loss decreased (-0.425100 --> -0.426180).  Saving model ...\n",
      "Validation loss decreased (-0.426180 --> -0.427200).  Saving model ...\n",
      "Validation loss decreased (-0.427200 --> -0.428180).  Saving model ...\n",
      "Validation loss decreased (-0.428180 --> -0.429120).  Saving model ...\n",
      "Validation loss decreased (-0.429120 --> -0.430010).  Saving model ...\n",
      "Epoch: 100, loss: -0.49471 valid loss:  -0.43086 lr: 0.00100 \n",
      "Validation loss decreased (-0.430010 --> -0.430860).  Saving model ...\n",
      "Validation loss decreased (-0.430860 --> -0.431680).  Saving model ...\n",
      "Validation loss decreased (-0.431680 --> -0.432460).  Saving model ...\n",
      "Validation loss decreased (-0.432460 --> -0.433200).  Saving model ...\n",
      "Validation loss decreased (-0.433200 --> -0.433920).  Saving model ...\n",
      "Validation loss decreased (-0.433920 --> -0.434600).  Saving model ...\n",
      "Validation loss decreased (-0.434600 --> -0.435260).  Saving model ...\n",
      "Validation loss decreased (-0.435260 --> -0.435890).  Saving model ...\n",
      "Validation loss decreased (-0.435890 --> -0.436490).  Saving model ...\n",
      "Validation loss decreased (-0.436490 --> -0.437070).  Saving model ...\n",
      "Epoch: 110, loss: -0.50395 valid loss:  -0.43762 lr: 0.00100 \n",
      "Validation loss decreased (-0.437070 --> -0.437620).  Saving model ...\n",
      "Validation loss decreased (-0.437620 --> -0.438150).  Saving model ...\n",
      "Validation loss decreased (-0.438150 --> -0.438660).  Saving model ...\n",
      "Validation loss decreased (-0.438660 --> -0.439150).  Saving model ...\n",
      "Validation loss decreased (-0.439150 --> -0.439630).  Saving model ...\n",
      "Validation loss decreased (-0.439630 --> -0.440080).  Saving model ...\n",
      "Validation loss decreased (-0.440080 --> -0.440520).  Saving model ...\n",
      "Validation loss decreased (-0.440520 --> -0.440940).  Saving model ...\n",
      "Validation loss decreased (-0.440940 --> -0.441340).  Saving model ...\n",
      "Validation loss decreased (-0.441340 --> -0.441730).  Saving model ...\n",
      "Epoch: 120, loss: -0.51007 valid loss:  -0.44211 lr: 0.00100 \n",
      "Validation loss decreased (-0.441730 --> -0.442110).  Saving model ...\n",
      "Validation loss decreased (-0.442110 --> -0.442470).  Saving model ...\n",
      "Validation loss decreased (-0.442470 --> -0.442820).  Saving model ...\n",
      "Validation loss decreased (-0.442820 --> -0.443150).  Saving model ...\n",
      "Validation loss decreased (-0.443150 --> -0.443480).  Saving model ...\n",
      "Validation loss decreased (-0.443480 --> -0.443790).  Saving model ...\n",
      "Validation loss decreased (-0.443790 --> -0.444100).  Saving model ...\n",
      "Validation loss decreased (-0.444100 --> -0.444390).  Saving model ...\n",
      "Validation loss decreased (-0.444390 --> -0.444670).  Saving model ...\n",
      "Validation loss decreased (-0.444670 --> -0.444950).  Saving model ...\n",
      "Epoch: 130, loss: -0.51431 valid loss:  -0.44521 lr: 0.00100 \n",
      "Validation loss decreased (-0.444950 --> -0.445210).  Saving model ...\n",
      "Validation loss decreased (-0.445210 --> -0.445470).  Saving model ...\n",
      "Validation loss decreased (-0.445470 --> -0.445720).  Saving model ...\n",
      "Validation loss decreased (-0.445720 --> -0.445960).  Saving model ...\n",
      "Validation loss decreased (-0.445960 --> -0.446190).  Saving model ...\n",
      "Validation loss decreased (-0.446190 --> -0.446420).  Saving model ...\n",
      "Validation loss decreased (-0.446420 --> -0.446640).  Saving model ...\n",
      "Validation loss decreased (-0.446640 --> -0.446850).  Saving model ...\n",
      "Validation loss decreased (-0.446850 --> -0.447050).  Saving model ...\n",
      "Validation loss decreased (-0.447050 --> -0.447250).  Saving model ...\n",
      "Epoch: 140, loss: -0.51736 valid loss:  -0.44745 lr: 0.00100 \n",
      "Validation loss decreased (-0.447250 --> -0.447450).  Saving model ...\n",
      "Validation loss decreased (-0.447450 --> -0.447640).  Saving model ...\n",
      "Validation loss decreased (-0.447640 --> -0.447820).  Saving model ...\n",
      "Validation loss decreased (-0.447820 --> -0.448000).  Saving model ...\n",
      "Validation loss decreased (-0.448000 --> -0.448170).  Saving model ...\n",
      "Validation loss decreased (-0.448170 --> -0.448340).  Saving model ...\n",
      "Validation loss decreased (-0.448340 --> -0.448500).  Saving model ...\n",
      "Validation loss decreased (-0.448500 --> -0.448660).  Saving model ...\n",
      "Validation loss decreased (-0.448660 --> -0.448820).  Saving model ...\n",
      "Validation loss decreased (-0.448820 --> -0.448970).  Saving model ...\n",
      "Epoch: 150, loss: -0.51962 valid loss:  -0.44911 lr: 0.00100 \n",
      "Validation loss decreased (-0.448970 --> -0.449110).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "from dev.pytorchtools import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience = 30, verbose = True)\n",
    "\n",
    "num_epochs = 150\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "# quantile = 0.3\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "val_acc_max = 0.75\n",
    "val_loss_min = 0.25\n",
    "\n",
    "lstm = Model1()\n",
    "\n",
    "# criterion = quantile_loss    # mean-squared error for regression\n",
    "criterion = torch.nn.NLLLoss()   # mean-squared error for regression\n",
    "# optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay= 1e-5)\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "m = nn.Softmax(dim = 1)\n",
    "\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate,momentum=0.9)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience= 60, \n",
    "#           factor =0.1 ,min_lr=1e-6, eps=1e-08)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,700,900,1000], gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "       losses = []\n",
    "       acc_list = []\n",
    "       for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "              lstm.train()\n",
    "              outputs = lstm(input.to(device))\n",
    "              optimizer.zero_grad()\n",
    "              \n",
    "              target = target.long()\n",
    "              loss = criterion(m(outputs), target.type(torch.long).to(device))\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "              losses.append(loss.item())\n",
    "              preds = torch.argmax(outputs, dim=1)\n",
    "              batch_acc = (preds == target).float().mean()\n",
    "              acc_list.append(batch_acc.item())\n",
    "           \n",
    "\n",
    "       train_losses.append(np.mean(np.array(losses)))\n",
    "     \n",
    "       losses = []\n",
    "       acc_list = []\n",
    "       for i, (input, target) in enumerate(val_loader):\n",
    "              lstm.eval()\n",
    "              with torch.no_grad():\n",
    "                     valid = lstm(input.to(device))\n",
    "                     target = target.long()\n",
    "                     val_loss = criterion(m(valid), target.type(torch.long).to(device))\n",
    "                     preds = torch.argmax(valid, dim=1)\n",
    "                     batch_acc = (preds == target).float().mean()\n",
    "                     losses.append(val_loss.item())\n",
    "                     acc_list.append(batch_acc.item())\n",
    "                     if val_acc_max < batch_acc.item():\n",
    "                            val_acc_max = batch_acc.item()\n",
    "                            print('Model saved, model val acc: ', batch_acc.item())\n",
    "                            \n",
    "                            torch.save(lstm, fr'C:\\Users\\USER\\Desktop\\19rne\\2023-RnE-main\\save_by_acc\\model_{epoch}.pth')\n",
    "\n",
    "                     if val_loss_min > val_loss:\n",
    "                            val_loss_min = val_loss\n",
    "                            torch.save(lstm, fr'C:\\Users\\USER\\Desktop\\19rne\\2023-RnE-main\\save_by_loss\\model_{epoch}.pth')\n",
    "                     \n",
    "\n",
    "       valid_losses.append(np.mean(np.array(losses)))\n",
    "       valid_accs.append(np.mean(np.array(losses)))\n",
    "\n",
    "\n",
    "\n",
    "       if epoch % 10 == 0:\n",
    "              # print(criterion1(outputs, y_train.to(device),quantile))\n",
    "\n",
    "              print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                             optimizer.param_groups[0][\"lr\"]))\n",
    "       \n",
    "       \n",
    "       # model.load_state_dict(torch.load(SAVEPATH+'model_weight.pth'))\n",
    "\n",
    "       # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "       # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "       early_stopping(round(valid_losses[-1],5), lstm)\n",
    "\n",
    "       if early_stopping.early_stop:\n",
    "              print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f lr: %1.5f \" %(epoch, train_losses[-1],valid_losses[-1],\n",
    "                                                                             optimizer.param_groups[0][\"lr\"]))\n",
    "              break\n",
    "           \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:15:28.128607800Z",
     "start_time": "2023-12-13T10:08:04.548877100Z"
    }
   },
   "id": "21dbba28f07a11d8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('lstm1.weight_ih_l0', tensor([[ 0.0106,  0.0242, -0.0078,  ...,  0.0666, -0.0148, -0.0841],\n",
      "        [-0.0231, -0.0550,  0.0528,  ...,  0.0713, -0.0040, -0.0330],\n",
      "        [ 0.0231,  0.0690,  0.0860,  ...,  0.0452,  0.0325,  0.0387],\n",
      "        ...,\n",
      "        [ 0.0411, -0.0150,  0.0160,  ...,  0.0471, -0.0437, -0.0269],\n",
      "        [ 0.0147, -0.0360, -0.0769,  ..., -0.0176, -0.0588, -0.0270],\n",
      "        [-0.0545,  0.0651,  0.0564,  ..., -0.0187,  0.0805, -0.0175]])), ('lstm1.weight_hh_l0', tensor([[ 0.0696, -0.0242,  0.0037,  ...,  0.0748, -0.0340,  0.0232],\n",
      "        [-0.0830,  0.0406, -0.0691,  ..., -0.0431, -0.0609, -0.0500],\n",
      "        [ 0.0299,  0.0226,  0.0513,  ...,  0.0128,  0.0134,  0.0247],\n",
      "        ...,\n",
      "        [-0.0672,  0.0084, -0.0638,  ..., -0.0733, -0.0641, -0.0737],\n",
      "        [ 0.0594,  0.0357,  0.0599,  ...,  0.0397, -0.0346, -0.0428],\n",
      "        [-0.0431,  0.0424, -0.0633,  ..., -0.0643,  0.0663,  0.0109]])), ('lstm1.bias_ih_l0', tensor([-0.0511, -0.0311, -0.0554,  0.0446,  0.0198,  0.0584, -0.0338,  0.0323,\n",
      "         0.0234, -0.0179,  0.0301,  0.0252, -0.0205, -0.0128, -0.0665, -0.0387,\n",
      "         0.0821, -0.0238,  0.0094, -0.0196, -0.0577,  0.0713,  0.0605, -0.0628,\n",
      "         0.0839,  0.0688,  0.0036,  0.0668,  0.0339,  0.0798,  0.0400,  0.0373,\n",
      "        -0.0638,  0.0360,  0.0832, -0.0033, -0.0445, -0.0572, -0.0005,  0.0089,\n",
      "        -0.0321,  0.0092, -0.0852, -0.0431, -0.0259,  0.0853,  0.0068, -0.0140,\n",
      "        -0.0008, -0.0415, -0.0811, -0.0454,  0.0833, -0.0664,  0.0190,  0.0169,\n",
      "         0.0460,  0.0584,  0.0322,  0.0822, -0.0080, -0.0122, -0.0322, -0.0659,\n",
      "         0.0843, -0.0072,  0.0047, -0.0860,  0.0726,  0.0479, -0.0055, -0.0010,\n",
      "         0.0311,  0.0812,  0.0264,  0.0446,  0.0503, -0.0380,  0.0647, -0.0573,\n",
      "         0.0876,  0.0543, -0.0776,  0.0011, -0.0758,  0.0667,  0.0683, -0.0582,\n",
      "         0.0731,  0.0621, -0.0375,  0.0455,  0.0045,  0.0872, -0.0873, -0.0085,\n",
      "         0.0670, -0.0802,  0.0701, -0.0452,  0.0049,  0.0629, -0.0007,  0.0677,\n",
      "         0.0620,  0.0244, -0.0835, -0.0668,  0.0257,  0.0587,  0.0329, -0.0868,\n",
      "        -0.0227,  0.0833, -0.0031, -0.0855, -0.0060,  0.0295,  0.0721, -0.0455,\n",
      "        -0.0099,  0.0556, -0.0407, -0.0584, -0.0701,  0.0511, -0.0266, -0.0211,\n",
      "        -0.0602, -0.0665, -0.0027, -0.0695, -0.0812, -0.0532, -0.0250,  0.0224,\n",
      "         0.0628, -0.0716,  0.0155, -0.0462,  0.0394, -0.0427,  0.0620,  0.0321,\n",
      "        -0.0167,  0.0031, -0.0615,  0.0650, -0.0794,  0.0601, -0.0363,  0.0877,\n",
      "         0.0235,  0.0786,  0.0032,  0.0858, -0.0696, -0.0049, -0.0136,  0.0367,\n",
      "         0.0755,  0.0756, -0.0705, -0.0431, -0.0190, -0.0067, -0.0350,  0.0454,\n",
      "        -0.0678, -0.0865,  0.0759, -0.0464, -0.0183,  0.0610, -0.0177,  0.0144,\n",
      "         0.0663,  0.0463,  0.0228,  0.0049,  0.0491,  0.0144,  0.0555, -0.0440,\n",
      "        -0.0513,  0.0466,  0.0558,  0.0030, -0.0727,  0.0234,  0.0389,  0.0335,\n",
      "        -0.0655, -0.0163,  0.0397,  0.0283, -0.0509,  0.0067,  0.0263,  0.0833,\n",
      "         0.0514, -0.0606, -0.0016, -0.0757,  0.0138,  0.0357, -0.0312,  0.0271,\n",
      "         0.0164, -0.0584,  0.0003,  0.0555, -0.0375, -0.0271,  0.0416,  0.0767,\n",
      "        -0.0760,  0.0418, -0.0367, -0.0168,  0.0251,  0.0124, -0.0867, -0.0056,\n",
      "         0.0873,  0.0631,  0.0424, -0.0431,  0.0123,  0.0673,  0.0675,  0.0094,\n",
      "         0.0004, -0.0550, -0.0036,  0.0875, -0.0870,  0.0387,  0.0683,  0.0879,\n",
      "         0.0407,  0.0675, -0.0752, -0.0738,  0.0352, -0.0606,  0.0244,  0.0865,\n",
      "         0.0685,  0.0756,  0.0755, -0.0234,  0.0018,  0.0534, -0.0549,  0.0038,\n",
      "        -0.0150, -0.0371, -0.0017,  0.0835, -0.0740,  0.0296,  0.0559,  0.0757,\n",
      "         0.0399,  0.0171,  0.0430, -0.0810, -0.0266, -0.0598,  0.0279,  0.0881,\n",
      "        -0.0093,  0.0353, -0.0228,  0.0727,  0.0581, -0.0460,  0.0715,  0.0028,\n",
      "         0.0450, -0.0038,  0.0505, -0.0202, -0.0293, -0.0328, -0.0733, -0.0077,\n",
      "        -0.0674,  0.0587,  0.0470,  0.0321, -0.0692, -0.0812, -0.0260,  0.0830,\n",
      "         0.0830, -0.0098,  0.0446,  0.0044,  0.0521, -0.0786, -0.0013, -0.0466,\n",
      "         0.0240,  0.0340, -0.0408,  0.0221, -0.0012, -0.0557,  0.0814,  0.0489,\n",
      "        -0.0104, -0.0767, -0.0107,  0.0367,  0.0752, -0.0586,  0.0261, -0.0701,\n",
      "        -0.0588,  0.0670, -0.0482, -0.0226, -0.0878,  0.0762, -0.0433, -0.0387,\n",
      "        -0.0215, -0.0131,  0.0819,  0.0084, -0.0278,  0.0361, -0.0730,  0.0602,\n",
      "         0.0274,  0.0871, -0.0575, -0.0178,  0.0336, -0.0710,  0.0847, -0.0531,\n",
      "        -0.0754,  0.0093, -0.0497,  0.0088, -0.0230,  0.0524, -0.0646,  0.0550,\n",
      "        -0.0050, -0.0783,  0.0456, -0.0240, -0.0393, -0.0135, -0.0165, -0.0690,\n",
      "        -0.0237,  0.0213, -0.0201, -0.0817, -0.0831, -0.0578,  0.0675, -0.0323,\n",
      "         0.0832,  0.0063,  0.0087, -0.0795, -0.0681, -0.0204,  0.0853,  0.0058,\n",
      "        -0.0830,  0.0487,  0.0391, -0.0275, -0.0040, -0.0810, -0.0737, -0.0262,\n",
      "        -0.0216,  0.0301, -0.0786,  0.0272, -0.0181,  0.0230, -0.0781,  0.0755,\n",
      "        -0.0681, -0.0413, -0.0622, -0.0845, -0.0304, -0.0764, -0.0542, -0.0794,\n",
      "        -0.0414, -0.0347, -0.0184, -0.0825, -0.0452, -0.0002,  0.0367,  0.0252,\n",
      "         0.0381,  0.0278,  0.0050,  0.0775, -0.0683, -0.0736, -0.0517, -0.0547,\n",
      "         0.0029, -0.0528,  0.0748,  0.0150, -0.0502, -0.0641,  0.0549,  0.0771,\n",
      "         0.0866, -0.0543, -0.0766, -0.0599, -0.0381, -0.0155, -0.0605, -0.0346,\n",
      "        -0.0713,  0.0867, -0.0094, -0.0560,  0.0312,  0.0341,  0.0830, -0.0552,\n",
      "        -0.0183,  0.0816,  0.0780,  0.0525, -0.0457, -0.0559,  0.0052,  0.0082,\n",
      "        -0.0138, -0.0348, -0.0596, -0.0663,  0.0808,  0.0511,  0.0197, -0.0789,\n",
      "        -0.0293,  0.0778,  0.0164, -0.0789,  0.0440,  0.0229,  0.0509,  0.0130,\n",
      "        -0.0594,  0.0073,  0.0460,  0.0293, -0.0059,  0.0092,  0.0399, -0.0563,\n",
      "        -0.0872, -0.0853,  0.0646, -0.0496,  0.0088,  0.0651,  0.0599, -0.0344,\n",
      "        -0.0730, -0.0031,  0.0829,  0.0105, -0.0278, -0.0580, -0.0031,  0.0019,\n",
      "        -0.0024,  0.0572, -0.0094,  0.0059,  0.0132,  0.0209,  0.0118,  0.0101,\n",
      "         0.0591,  0.0347, -0.0393,  0.0694,  0.0449,  0.0645, -0.0804,  0.0857,\n",
      "         0.0040,  0.0661, -0.0614, -0.0095, -0.0575, -0.0616,  0.0548, -0.0640])), ('lstm1.bias_hh_l0', tensor([-0.0474, -0.0371,  0.0881,  0.0340, -0.0575,  0.0009, -0.0239, -0.0294,\n",
      "         0.0447, -0.0761,  0.0123,  0.0480, -0.0062, -0.0536,  0.0132, -0.0093,\n",
      "         0.0059, -0.0150,  0.0142, -0.0054,  0.0071, -0.0728, -0.0614, -0.0869,\n",
      "         0.0561,  0.0517, -0.0394,  0.0468, -0.0164,  0.0384, -0.0548, -0.0199,\n",
      "        -0.0183,  0.0416, -0.0506, -0.0724,  0.0666,  0.0486,  0.0036, -0.0562,\n",
      "         0.0087, -0.0529,  0.0381,  0.0851,  0.0229, -0.0865, -0.0702,  0.0006,\n",
      "         0.0778, -0.0232, -0.0853, -0.0503, -0.0592,  0.0039,  0.0310, -0.0130,\n",
      "         0.0078, -0.0519, -0.0845, -0.0014, -0.0511, -0.0057, -0.0064,  0.0607,\n",
      "        -0.0341,  0.0678, -0.0502, -0.0058, -0.0455, -0.0176,  0.0565,  0.0725,\n",
      "        -0.0256,  0.0466,  0.0368,  0.0073, -0.0651, -0.0475, -0.0524,  0.0626,\n",
      "         0.0425,  0.0372,  0.0415, -0.0160,  0.0019, -0.0228, -0.0492, -0.0868,\n",
      "        -0.0067,  0.0562, -0.0685, -0.0880,  0.0090, -0.0583,  0.0584, -0.0106,\n",
      "         0.0640, -0.0394, -0.0840,  0.0477, -0.0363,  0.0505, -0.0684,  0.0529,\n",
      "        -0.0241,  0.0639,  0.0169, -0.0141, -0.0224,  0.0190,  0.0766, -0.0152,\n",
      "         0.0884,  0.0156,  0.0011, -0.0345, -0.0738,  0.0038,  0.0699, -0.0164,\n",
      "         0.0128, -0.0305, -0.0283, -0.0844, -0.0017,  0.0617, -0.0383, -0.0329,\n",
      "         0.0137,  0.0604, -0.0373,  0.0565,  0.0344, -0.0330,  0.0609,  0.0275,\n",
      "         0.0344, -0.0751, -0.0224,  0.0562, -0.0760,  0.0075,  0.0557,  0.0265,\n",
      "         0.0500, -0.0111,  0.0170,  0.0349,  0.0014,  0.0088,  0.0372,  0.0325,\n",
      "        -0.0522,  0.0371,  0.0011, -0.0380,  0.0608, -0.0446, -0.0208,  0.0215,\n",
      "         0.0621,  0.0395,  0.0696,  0.0878, -0.0675, -0.0731,  0.0590,  0.0705,\n",
      "        -0.0470,  0.0045, -0.0624,  0.0278,  0.0346,  0.0150, -0.0120,  0.0627,\n",
      "         0.0844,  0.0883,  0.0351,  0.0353, -0.0280, -0.0536,  0.0007,  0.0819,\n",
      "         0.0683, -0.0083, -0.0260, -0.0526,  0.0771, -0.0727,  0.0125,  0.0674,\n",
      "        -0.0581, -0.0846, -0.0607,  0.0565,  0.0002,  0.0597, -0.0206,  0.0130,\n",
      "         0.0118, -0.0652,  0.0848, -0.0455, -0.0691,  0.0087,  0.0480,  0.0733,\n",
      "        -0.0750,  0.0491,  0.0329, -0.0484,  0.0718, -0.0313, -0.0517, -0.0792,\n",
      "        -0.0481, -0.0021,  0.0170, -0.0225, -0.0163,  0.0489, -0.0215,  0.0324,\n",
      "         0.0123, -0.0713,  0.0787, -0.0472,  0.0577, -0.0441, -0.0594,  0.0646,\n",
      "        -0.0723, -0.0387, -0.0598,  0.0559, -0.0700, -0.0828,  0.0208,  0.0462,\n",
      "         0.0753,  0.0033, -0.0062,  0.0873, -0.0443, -0.0646,  0.0363,  0.0243,\n",
      "        -0.0588,  0.0340, -0.0668, -0.0754, -0.0355, -0.0793,  0.0116,  0.0005,\n",
      "         0.0554, -0.0612, -0.0084,  0.0493, -0.0087,  0.0841,  0.0161,  0.0307,\n",
      "         0.0068, -0.0320, -0.0317,  0.0247,  0.0576,  0.0726, -0.0280,  0.0884,\n",
      "        -0.0303,  0.0036,  0.0606,  0.0013, -0.0477,  0.0628,  0.0007,  0.0220,\n",
      "         0.0733, -0.0486,  0.0480, -0.0384, -0.0686,  0.0537,  0.0091, -0.0803,\n",
      "         0.0365, -0.0074, -0.0020, -0.0084, -0.0101, -0.0732, -0.0739, -0.0508,\n",
      "        -0.0655, -0.0645, -0.0777, -0.0421,  0.0573,  0.0513,  0.0421,  0.0138,\n",
      "         0.0416, -0.0190, -0.0132, -0.0354, -0.0779,  0.0669,  0.0015, -0.0135,\n",
      "        -0.0175, -0.0424,  0.0027,  0.0572,  0.0321,  0.0587, -0.0614, -0.0487,\n",
      "        -0.0381, -0.0516, -0.0767, -0.0867,  0.0519,  0.0714,  0.0467,  0.0513,\n",
      "         0.0545,  0.0476,  0.0236,  0.0313, -0.0733, -0.0501,  0.0040,  0.0364,\n",
      "         0.0672, -0.0847, -0.0114, -0.0332,  0.0654, -0.0407, -0.0574,  0.0729,\n",
      "         0.0310,  0.0083, -0.0152, -0.0637, -0.0242,  0.0071,  0.0524, -0.0349,\n",
      "         0.0791, -0.0078,  0.0491,  0.0011, -0.0551,  0.0411,  0.0499, -0.0632,\n",
      "        -0.0379,  0.0817,  0.0407, -0.0455, -0.0010, -0.0292, -0.0837,  0.0841,\n",
      "        -0.0273, -0.0334,  0.0352, -0.0719, -0.0378,  0.0477,  0.0089, -0.0837,\n",
      "        -0.0346,  0.0610,  0.0280,  0.0146,  0.0262,  0.0181, -0.0595,  0.0863,\n",
      "         0.0364, -0.0211,  0.0208, -0.0436,  0.0007,  0.0334, -0.0426,  0.0011,\n",
      "        -0.0704,  0.0514, -0.0232, -0.0033, -0.0870, -0.0497,  0.0613, -0.0489,\n",
      "         0.0070, -0.0300,  0.0782, -0.0072, -0.0484, -0.0110,  0.0175,  0.0681,\n",
      "        -0.0243, -0.0099, -0.0417, -0.0365, -0.0808, -0.0520, -0.0270, -0.0781,\n",
      "         0.0196,  0.0529, -0.0253, -0.0309, -0.0765,  0.0394,  0.0768,  0.0298,\n",
      "         0.0298,  0.0612,  0.0131,  0.0674, -0.0445,  0.0859,  0.0211, -0.0139,\n",
      "        -0.0502, -0.0284, -0.0430,  0.0137,  0.0466, -0.0055,  0.0571, -0.0449,\n",
      "        -0.0153, -0.0326, -0.0143, -0.0878,  0.0295, -0.0868,  0.0645, -0.0050,\n",
      "        -0.0088,  0.0464, -0.0076,  0.0846,  0.0463,  0.0477, -0.0654, -0.0449,\n",
      "        -0.0807,  0.0779,  0.0026,  0.0249,  0.0108, -0.0465,  0.0789, -0.0646,\n",
      "         0.0035, -0.0444, -0.0190, -0.0506,  0.0883, -0.0271,  0.0854,  0.0522,\n",
      "         0.0829, -0.0789, -0.0767, -0.0016,  0.0522,  0.0612, -0.0170,  0.0046,\n",
      "         0.0724,  0.0031, -0.0700, -0.0426,  0.0397, -0.0256,  0.0150, -0.0383,\n",
      "        -0.0878,  0.0332, -0.0869,  0.0556,  0.0061, -0.0614,  0.0212,  0.0070,\n",
      "        -0.0637, -0.0711,  0.0361,  0.0204,  0.0547,  0.0066, -0.0103,  0.0688,\n",
      "         0.0724,  0.0139, -0.0634,  0.0275,  0.0601, -0.0561, -0.0220, -0.0168])), ('lstm2.weight_ih_l0', tensor([[ 0.0506, -0.0346,  0.0083,  ..., -0.0026, -0.0375, -0.0273],\n",
      "        [-0.0457, -0.0460,  0.0050,  ...,  0.0555,  0.0476, -0.0005],\n",
      "        [ 0.0048,  0.0332,  0.0444,  ..., -0.0057, -0.0275,  0.0437],\n",
      "        ...,\n",
      "        [ 0.0364,  0.0456,  0.0406,  ..., -0.0095,  0.0357,  0.0600],\n",
      "        [ 0.0519,  0.0305, -0.0226,  ..., -0.0379,  0.0335,  0.0416],\n",
      "        [ 0.0572,  0.0136,  0.0226,  ..., -0.0236, -0.0320,  0.0059]])), ('lstm2.weight_hh_l0', tensor([[-0.0407, -0.0177, -0.0074,  ...,  0.0135, -0.0146,  0.0037],\n",
      "        [ 0.0404, -0.0331, -0.0475,  ..., -0.0329, -0.0259, -0.0376],\n",
      "        [ 0.0067, -0.0357,  0.0608,  ..., -0.0309,  0.0617,  0.0462],\n",
      "        ...,\n",
      "        [ 0.0114, -0.0520, -0.0312,  ...,  0.0596, -0.0048,  0.0085],\n",
      "        [-0.0513,  0.0332,  0.0461,  ...,  0.0073,  0.0338,  0.0063],\n",
      "        [-0.0236,  0.0019, -0.0426,  ..., -0.0535,  0.0421,  0.0346]])), ('lstm2.bias_ih_l0', tensor([-0.0388, -0.0319, -0.0331,  ...,  0.0199, -0.0471,  0.0084])), ('lstm2.bias_hh_l0', tensor([ 0.0382,  0.0548,  0.0523,  ...,  0.0528, -0.0007,  0.0022])), ('lstm3.weight_ih_l0', tensor([[-0.0111,  0.0022, -0.0019,  ..., -0.0061,  0.0013, -0.0256],\n",
      "        [-0.0239,  0.0101, -0.0047,  ..., -0.0339,  0.0299,  0.0113],\n",
      "        [ 0.0256, -0.0254,  0.0154,  ..., -0.0179,  0.0429, -0.0410],\n",
      "        ...,\n",
      "        [-0.0211, -0.0123,  0.0096,  ..., -0.0435, -0.0227, -0.0264],\n",
      "        [ 0.0035,  0.0300,  0.0269,  ..., -0.0231, -0.0401,  0.0213],\n",
      "        [-0.0261, -0.0020, -0.0015,  ..., -0.0018, -0.0344, -0.0422]])), ('lstm3.weight_hh_l0', tensor([[ 0.0181,  0.0097,  0.0210,  ..., -0.0441, -0.0323,  0.0035],\n",
      "        [ 0.0068,  0.0199,  0.0307,  ..., -0.0291, -0.0247,  0.0006],\n",
      "        [ 0.0197,  0.0086,  0.0083,  ...,  0.0291, -0.0019,  0.0383],\n",
      "        ...,\n",
      "        [ 0.0237, -0.0010, -0.0179,  ..., -0.0034, -0.0176,  0.0359],\n",
      "        [-0.0113,  0.0211,  0.0116,  ..., -0.0302, -0.0231, -0.0121],\n",
      "        [ 0.0120,  0.0166,  0.0371,  ...,  0.0265, -0.0079,  0.0337]])), ('lstm3.bias_ih_l0', tensor([ 0.0192, -0.0407,  0.0370,  ...,  0.0318, -0.0151,  0.0244])), ('lstm3.bias_hh_l0', tensor([-0.0128,  0.0228,  0.0361,  ..., -0.0208,  0.0404, -0.0173])), ('lstm4.weight_ih_l0', tensor([[-0.0375,  0.0435,  0.0312,  ...,  0.0304,  0.0120, -0.0081],\n",
      "        [ 0.0601,  0.0487, -0.0558,  ...,  0.0032, -0.0222,  0.0543],\n",
      "        [-0.0162, -0.0367,  0.0150,  ...,  0.0373, -0.0587, -0.0520],\n",
      "        ...,\n",
      "        [-0.0141, -0.0123,  0.0286,  ..., -0.0150,  0.0365, -0.0138],\n",
      "        [ 0.0263, -0.0575, -0.0535,  ...,  0.0018, -0.0478,  0.0185],\n",
      "        [-0.0024,  0.0612,  0.0541,  ..., -0.0066, -0.0355, -0.0577]])), ('lstm4.weight_hh_l0', tensor([[-0.0583, -0.0566, -0.0378,  ..., -0.0314, -0.0073, -0.0437],\n",
      "        [-0.0367,  0.0427,  0.0242,  ..., -0.0506, -0.0114, -0.0297],\n",
      "        [-0.0580,  0.0025, -0.0325,  ..., -0.0498,  0.0546,  0.0530],\n",
      "        ...,\n",
      "        [ 0.0306, -0.0291, -0.0406,  ..., -0.0144, -0.0301, -0.0419],\n",
      "        [-0.0561, -0.0117,  0.0304,  ..., -0.0080,  0.0619, -0.0288],\n",
      "        [ 0.0451,  0.0486,  0.0116,  ..., -0.0622, -0.0039,  0.0035]])), ('lstm4.bias_ih_l0', tensor([ 0.0322, -0.0167, -0.0278,  ...,  0.0395, -0.0593,  0.0543])), ('lstm4.bias_hh_l0', tensor([-0.0020,  0.0195, -0.0286,  ...,  0.0124,  0.0291,  0.0422])), ('lstm5.weight_ih_l0', tensor([[ 0.0772,  0.0388, -0.0227,  ...,  0.0587,  0.0386, -0.0274],\n",
      "        [-0.0663, -0.0167,  0.0184,  ..., -0.0521, -0.0406, -0.0676],\n",
      "        [-0.0210,  0.0463,  0.0741,  ..., -0.0339,  0.0579, -0.0723],\n",
      "        ...,\n",
      "        [ 0.0745, -0.0629, -0.0183,  ...,  0.0189, -0.0763, -0.0293],\n",
      "        [ 0.0099, -0.0383,  0.0002,  ..., -0.0256,  0.0484,  0.0784],\n",
      "        [-0.0373,  0.0344, -0.0086,  ..., -0.0114, -0.0786, -0.0339]])), ('lstm5.weight_hh_l0', tensor([[-0.0858, -0.0862,  0.0681,  ..., -0.0365,  0.0645,  0.0525],\n",
      "        [-0.0254,  0.0736,  0.0442,  ..., -0.0115, -0.0558,  0.0425],\n",
      "        [ 0.0781, -0.0252, -0.0187,  ..., -0.0841, -0.0796,  0.0367],\n",
      "        ...,\n",
      "        [ 0.0576,  0.0803,  0.0305,  ..., -0.0375, -0.0149,  0.0799],\n",
      "        [ 0.0207,  0.0779,  0.0862,  ..., -0.0578,  0.0252,  0.0269],\n",
      "        [-0.0678,  0.0310, -0.0599,  ..., -0.0208,  0.0162,  0.0856]])), ('lstm5.bias_ih_l0', tensor([ 0.0685,  0.0805, -0.0317, -0.0142, -0.0268, -0.0264,  0.0564,  0.0182,\n",
      "        -0.0574,  0.0232,  0.0049, -0.0103, -0.0208, -0.0647, -0.0196, -0.0854,\n",
      "         0.0748,  0.0549,  0.0377,  0.0523,  0.0168, -0.0716, -0.0425, -0.0374,\n",
      "        -0.0435,  0.0438,  0.0757, -0.0118,  0.0426,  0.0159,  0.0902, -0.0040,\n",
      "        -0.0469,  0.0367, -0.0652,  0.0170,  0.0586, -0.0501, -0.0525,  0.0353,\n",
      "        -0.0475,  0.0017,  0.0531,  0.0218,  0.0383, -0.0505,  0.0454, -0.0392,\n",
      "         0.0442,  0.0501,  0.0375,  0.0009, -0.0049, -0.0072, -0.0404,  0.0865,\n",
      "         0.0182,  0.0362, -0.0570, -0.0763,  0.0708, -0.0746, -0.0543, -0.0573,\n",
      "        -0.0416, -0.0391, -0.0713, -0.0212, -0.0854, -0.0755, -0.0069,  0.0859,\n",
      "        -0.0576,  0.0826,  0.0250, -0.0227, -0.0633,  0.0834, -0.0328,  0.0806,\n",
      "        -0.0162,  0.0650, -0.0242,  0.0772,  0.0824, -0.0247,  0.0826,  0.0857,\n",
      "        -0.0440,  0.0650, -0.0201, -0.0313, -0.0032,  0.0537, -0.0306,  0.0872,\n",
      "        -0.0631,  0.0063,  0.0649, -0.0756, -0.0557, -0.0626,  0.0769, -0.0206,\n",
      "        -0.0379, -0.0496, -0.0032,  0.0097, -0.0652, -0.0542, -0.0469, -0.0180,\n",
      "         0.0226,  0.0791, -0.0603,  0.0680,  0.0534, -0.0522,  0.0613,  0.0762,\n",
      "        -0.0252, -0.0791, -0.0467, -0.0229, -0.0792, -0.0284, -0.0601, -0.0157,\n",
      "        -0.0863, -0.0158,  0.0336,  0.0118, -0.0710, -0.0692,  0.0603, -0.0685,\n",
      "         0.0348,  0.0366,  0.0630,  0.0847, -0.0249,  0.0306,  0.0317, -0.0761,\n",
      "         0.0660,  0.0396, -0.0591,  0.0701,  0.0756,  0.0421, -0.0208, -0.0855,\n",
      "         0.0033, -0.0815, -0.0246, -0.0706,  0.0191, -0.0300, -0.0770, -0.0377,\n",
      "        -0.0465,  0.0283,  0.0629,  0.0484,  0.0431,  0.0649, -0.0284, -0.0584,\n",
      "         0.0250, -0.0461, -0.0330, -0.0450,  0.0356,  0.0464, -0.0432,  0.0846,\n",
      "         0.0298, -0.0346, -0.0440,  0.0585,  0.0663,  0.0301,  0.0834,  0.0377,\n",
      "        -0.0007,  0.0145, -0.0232, -0.0130, -0.0701, -0.0681, -0.0701,  0.0263,\n",
      "        -0.0401, -0.0200, -0.0567, -0.0110,  0.0007, -0.0866, -0.0300, -0.0531,\n",
      "         0.0051,  0.0798,  0.0339, -0.0874,  0.0051, -0.0589,  0.0353, -0.0043,\n",
      "        -0.0283, -0.0344, -0.0430,  0.0319,  0.0109,  0.0479, -0.0533,  0.0690,\n",
      "         0.0764, -0.0005, -0.0424,  0.0092,  0.0434,  0.0670,  0.0601, -0.0441,\n",
      "         0.0175,  0.0550,  0.0013, -0.0861, -0.0199, -0.0503,  0.0736,  0.0643,\n",
      "         0.0780,  0.0621, -0.0034, -0.0129,  0.0509,  0.0511,  0.0212,  0.0861,\n",
      "         0.0594,  0.0457, -0.0272, -0.0260, -0.0666,  0.0797,  0.0113, -0.0047,\n",
      "        -0.0067,  0.0110, -0.0792,  0.0067,  0.0185,  0.0621, -0.0521, -0.0741,\n",
      "        -0.0558, -0.0461, -0.0213,  0.0450, -0.0493,  0.0554, -0.0527, -0.0376,\n",
      "         0.0209, -0.0444,  0.0736,  0.0311, -0.0662,  0.0329,  0.0224, -0.0073,\n",
      "         0.0250,  0.0184,  0.0600,  0.0511, -0.0016,  0.0051,  0.0173,  0.0280,\n",
      "         0.0768, -0.0576,  0.0785,  0.0105, -0.0765, -0.0091, -0.0437, -0.0240,\n",
      "        -0.0859, -0.0835,  0.0006,  0.0775,  0.0379, -0.0331, -0.0488,  0.0565,\n",
      "         0.0189, -0.0168,  0.0220,  0.0627,  0.0409, -0.0503, -0.0683, -0.0214,\n",
      "        -0.0441, -0.0849,  0.0999, -0.0015, -0.0721,  0.0203,  0.0228,  0.1088,\n",
      "        -0.0137, -0.0264, -0.0054,  0.0377,  0.0069,  0.0093,  0.0890,  0.0113,\n",
      "         0.0481,  0.0145, -0.0523, -0.0506, -0.0401, -0.0779,  0.0548,  0.0784,\n",
      "        -0.0503, -0.0104, -0.0396,  0.0496, -0.0829, -0.0554,  0.0052,  0.0898,\n",
      "         0.0502,  0.0260, -0.0925, -0.0520, -0.0613,  0.0206,  0.0412, -0.0501,\n",
      "         0.0178,  0.0533,  0.0522, -0.0576, -0.0923, -0.0758, -0.0185,  0.0537,\n",
      "         0.0377,  0.0010, -0.0644,  0.0251,  0.0627,  0.0158,  0.0683, -0.0743,\n",
      "        -0.0626, -0.0605,  0.0443,  0.0565, -0.0348, -0.0032,  0.0299, -0.1061,\n",
      "         0.0213, -0.0483, -0.0224,  0.0568,  0.0192, -0.0402, -0.0373,  0.0721,\n",
      "         0.0272,  0.0495,  0.0398, -0.0709, -0.0723,  0.0638, -0.0494, -0.0246,\n",
      "        -0.0812, -0.0478,  0.0793, -0.0175,  0.0738,  0.0797,  0.0787, -0.0341,\n",
      "         0.0757,  0.0304,  0.0448, -0.0347,  0.0602, -0.0823, -0.0148, -0.0879,\n",
      "        -0.0741, -0.0145,  0.0595, -0.0587,  0.0512,  0.0066,  0.0390, -0.0403,\n",
      "         0.0138, -0.0237,  0.0738,  0.0753,  0.0829,  0.0519, -0.0262,  0.0724,\n",
      "        -0.0835, -0.0284,  0.0753,  0.0291,  0.0711, -0.0069,  0.0686, -0.0451,\n",
      "        -0.0142,  0.0773, -0.0392, -0.0638,  0.0709,  0.0099,  0.0248, -0.0368,\n",
      "         0.0014, -0.0297, -0.0540,  0.0308, -0.0665, -0.0833, -0.0254,  0.0830,\n",
      "        -0.0597, -0.0674,  0.0635,  0.0157, -0.0144,  0.0803,  0.0691, -0.0210,\n",
      "         0.0728, -0.0097,  0.0264,  0.0386,  0.0639, -0.0232, -0.0333, -0.0695,\n",
      "        -0.0744,  0.0308, -0.0818, -0.0414, -0.0074, -0.0194, -0.0773,  0.0109,\n",
      "        -0.0248, -0.0039, -0.0388, -0.0141, -0.0725, -0.0054, -0.0776,  0.0610,\n",
      "         0.0753, -0.0796, -0.0235, -0.0041,  0.0338,  0.0148,  0.0292, -0.0703,\n",
      "        -0.0818,  0.0348,  0.0209,  0.0407, -0.0104,  0.0368, -0.0536,  0.0539,\n",
      "         0.0566,  0.0285,  0.0029,  0.0342, -0.0859, -0.0784,  0.0831, -0.0639,\n",
      "        -0.0597, -0.0508, -0.0785,  0.0659,  0.0021,  0.0635,  0.0505,  0.0775,\n",
      "         0.0208,  0.0166,  0.0162, -0.0722,  0.0486,  0.0526, -0.0257,  0.0669])), ('lstm5.bias_hh_l0', tensor([-0.0849, -0.0635, -0.0137, -0.0188, -0.0589, -0.0661, -0.0353, -0.0078,\n",
      "        -0.0245, -0.0250,  0.0435,  0.0708,  0.0270, -0.0781, -0.0225,  0.0490,\n",
      "         0.0854, -0.0711,  0.0883, -0.0116, -0.0590,  0.0179,  0.0199,  0.0499,\n",
      "        -0.0638,  0.0363, -0.0437, -0.0269,  0.0087,  0.0496, -0.0246,  0.0315,\n",
      "        -0.0616, -0.0642, -0.0647,  0.0494, -0.0710,  0.0179, -0.0110, -0.0535,\n",
      "        -0.0246, -0.0456, -0.0698, -0.0495, -0.0307, -0.0827, -0.0655, -0.0543,\n",
      "        -0.0311, -0.0312,  0.0399,  0.0145,  0.0142,  0.0378,  0.0821, -0.0055,\n",
      "        -0.0703, -0.0189,  0.0785,  0.0408, -0.0087,  0.0195, -0.0632,  0.0401,\n",
      "        -0.0367,  0.0555,  0.0802,  0.0651, -0.0811,  0.0669, -0.0469, -0.0611,\n",
      "        -0.0320,  0.0221, -0.0729, -0.0183, -0.0416, -0.0255, -0.0813,  0.0878,\n",
      "         0.0789,  0.0037,  0.0248, -0.0200, -0.0870,  0.0638,  0.0564,  0.0275,\n",
      "         0.0375,  0.0072, -0.0403, -0.0498,  0.0823,  0.0229,  0.0101, -0.0177,\n",
      "         0.0204, -0.0342, -0.0283,  0.0150,  0.0311,  0.0553, -0.0204, -0.0414,\n",
      "        -0.0385, -0.0263, -0.0186, -0.0520,  0.0630, -0.0178,  0.0071,  0.0666,\n",
      "        -0.0294, -0.0263,  0.0332,  0.0684, -0.0224,  0.0529,  0.0126,  0.0424,\n",
      "        -0.0231, -0.0528,  0.0196, -0.0647,  0.0756,  0.0500,  0.0351,  0.0216,\n",
      "         0.0296,  0.0086,  0.0107,  0.0385, -0.0383,  0.0359,  0.0481,  0.0872,\n",
      "        -0.0116,  0.0128,  0.0711, -0.0683, -0.0721,  0.0866,  0.0434,  0.0608,\n",
      "        -0.0719,  0.0599, -0.0224, -0.0686,  0.0652,  0.0512,  0.0066,  0.0799,\n",
      "         0.0105,  0.0463,  0.0363, -0.0121,  0.0660,  0.0441,  0.0146,  0.0490,\n",
      "         0.0161, -0.0047,  0.0085, -0.0429,  0.0769,  0.0472, -0.0540,  0.0551,\n",
      "        -0.0438,  0.0682,  0.0641, -0.0254, -0.0881,  0.0794, -0.0808,  0.0714,\n",
      "        -0.0070,  0.0868,  0.0386,  0.0010, -0.0534,  0.0606,  0.0520, -0.0687,\n",
      "         0.0449,  0.0697, -0.0068,  0.0019, -0.0878, -0.0083,  0.0826, -0.0023,\n",
      "         0.0481,  0.0739, -0.0441,  0.0442, -0.0273, -0.0424, -0.0248, -0.0425,\n",
      "         0.0370, -0.0596,  0.0789, -0.0385,  0.0652,  0.0357, -0.0012,  0.0811,\n",
      "         0.0392,  0.0770,  0.0446, -0.0774,  0.0349, -0.0250,  0.0848, -0.0406,\n",
      "         0.0388, -0.0322,  0.0782,  0.0478,  0.0716,  0.0396,  0.0360,  0.0091,\n",
      "        -0.0244, -0.0627,  0.0684,  0.0142, -0.0811, -0.0148, -0.0851, -0.0361,\n",
      "        -0.0204,  0.0522,  0.0687, -0.0129, -0.0657,  0.0462,  0.0375,  0.0798,\n",
      "        -0.0356, -0.0078, -0.0502,  0.0393,  0.0554, -0.0825, -0.0004, -0.0329,\n",
      "        -0.0159, -0.0284,  0.0811,  0.0157,  0.0356,  0.0053, -0.0792, -0.0180,\n",
      "         0.0301,  0.0575,  0.0052, -0.0795,  0.0132,  0.0346, -0.0351,  0.0142,\n",
      "         0.0419,  0.0381,  0.0445, -0.0877,  0.0903,  0.0611, -0.0241, -0.0091,\n",
      "        -0.0023, -0.0648, -0.0050, -0.0386,  0.0469,  0.0696,  0.0043,  0.0517,\n",
      "         0.0150,  0.0700,  0.0658, -0.0874,  0.0336, -0.0818, -0.0554, -0.0348,\n",
      "         0.0032, -0.0912,  0.0937,  0.0354, -0.0453, -0.0311,  0.0748,  0.0011,\n",
      "         0.0054,  0.0205,  0.0461,  0.0297,  0.0019,  0.0831,  0.0314,  0.0245,\n",
      "        -0.0847,  0.0216,  0.0221,  0.0645, -0.0596,  0.0433,  0.0206,  0.0713,\n",
      "         0.0665,  0.0696, -0.0009, -0.0117,  0.0072,  0.0687,  0.0294,  0.0315,\n",
      "        -0.0521, -0.0419, -0.0045,  0.0072, -0.0593,  0.0131, -0.0334, -0.0461,\n",
      "        -0.0074,  0.0767, -0.0621,  0.0596, -0.0398,  0.0304,  0.0333, -0.0660,\n",
      "        -0.0324, -0.0546, -0.0573, -0.0383,  0.0247,  0.0393, -0.0475,  0.0978,\n",
      "         0.1022,  0.0135,  0.0808,  0.0346, -0.0336,  0.0421,  0.0392,  0.0729,\n",
      "         0.0031,  0.0317,  0.0640, -0.0206, -0.0912,  0.0862,  0.0071, -0.0240,\n",
      "        -0.0255,  0.0728,  0.0705, -0.0277, -0.0207, -0.0442, -0.0383, -0.0687,\n",
      "        -0.0416, -0.0221, -0.0056, -0.0583,  0.0963, -0.0646, -0.0709, -0.0357,\n",
      "        -0.0186,  0.0029, -0.0716,  0.0791, -0.0301,  0.0151, -0.0584,  0.0890,\n",
      "        -0.0356, -0.0620,  0.0838, -0.0876, -0.0528,  0.0096, -0.0589,  0.0760,\n",
      "         0.0686, -0.0699, -0.0061,  0.0414, -0.0628, -0.0665, -0.0230, -0.0309,\n",
      "        -0.0038, -0.0163,  0.0692,  0.0197,  0.0409, -0.0373,  0.0395,  0.0130,\n",
      "         0.0394, -0.0553,  0.0109,  0.0846, -0.0821, -0.0043, -0.0230, -0.0612,\n",
      "         0.0748,  0.0200, -0.0769,  0.0112, -0.0217,  0.0692, -0.0161, -0.0385,\n",
      "         0.0878, -0.0127,  0.0569, -0.0026, -0.0227,  0.0325,  0.0225, -0.0807,\n",
      "         0.0026, -0.0880, -0.0665, -0.0164,  0.0461,  0.0005, -0.0782, -0.0263,\n",
      "         0.0380, -0.0108, -0.0746, -0.0606, -0.0495,  0.0856,  0.0215, -0.0652,\n",
      "         0.0154, -0.0492, -0.0081, -0.0501,  0.0501,  0.0433,  0.0567, -0.0550,\n",
      "        -0.0737, -0.0323, -0.0202, -0.0491, -0.0099,  0.0843,  0.0081, -0.0806,\n",
      "        -0.0726, -0.0168, -0.0824, -0.0341,  0.0660, -0.0776,  0.0003,  0.0340,\n",
      "         0.0629, -0.0216,  0.0660, -0.0593,  0.0788, -0.0848, -0.0366, -0.0540,\n",
      "         0.0264,  0.0269, -0.0788, -0.0766, -0.0347,  0.0693,  0.0550, -0.0663,\n",
      "         0.0288,  0.0614, -0.0595,  0.0729,  0.0386,  0.0019, -0.0336,  0.0596,\n",
      "        -0.0609, -0.0153,  0.0275,  0.0269, -0.0472,  0.0031, -0.0447,  0.0476,\n",
      "         0.0779, -0.0852, -0.0515,  0.0342, -0.0740, -0.0601,  0.0327, -0.0089])), ('lstm6.weight_ih_l0', tensor([[ 0.0403, -0.0218, -0.0883,  ...,  0.0301,  0.0519,  0.0589],\n",
      "        [ 0.0376,  0.0242, -0.0281,  ..., -0.0672,  0.0395,  0.0262],\n",
      "        [-0.1013,  0.0586,  0.1038,  ...,  0.0455, -0.0900, -0.0098],\n",
      "        ...,\n",
      "        [-0.1198, -0.0736, -0.0147,  ..., -0.0378,  0.0588,  0.1152],\n",
      "        [-0.0659, -0.0190, -0.0510,  ..., -0.0565,  0.0948,  0.0102],\n",
      "        [-0.0643,  0.1238, -0.0814,  ..., -0.0789,  0.0127, -0.0818]])), ('lstm6.weight_hh_l0', tensor([[-0.0660, -0.0036,  0.0452,  ...,  0.0144,  0.1195,  0.0615],\n",
      "        [ 0.0580, -0.0979,  0.0650,  ..., -0.0320,  0.0202, -0.0460],\n",
      "        [ 0.0545, -0.0812, -0.0042,  ..., -0.1045,  0.0691,  0.0438],\n",
      "        ...,\n",
      "        [ 0.0796, -0.0083, -0.1197,  ...,  0.0082,  0.0588, -0.0281],\n",
      "        [-0.1066, -0.0148, -0.0196,  ...,  0.0281,  0.0730,  0.0310],\n",
      "        [-0.0042, -0.0703,  0.0542,  ...,  0.0205, -0.0774, -0.0962]])), ('lstm6.bias_ih_l0', tensor([ 0.0381, -0.0809,  0.0390,  0.0744, -0.0036,  0.0942,  0.0270,  0.0256,\n",
      "        -0.0217,  0.1185, -0.0544,  0.0197, -0.0861, -0.0376,  0.0019,  0.0800,\n",
      "        -0.0035,  0.0265,  0.0083, -0.0607,  0.1111, -0.1037,  0.0603,  0.1212,\n",
      "         0.0286,  0.0738,  0.0457, -0.0555, -0.0827,  0.1236,  0.0146,  0.0939,\n",
      "         0.0757,  0.0725, -0.1012, -0.1155, -0.0250,  0.0231,  0.1301,  0.0557,\n",
      "         0.1053,  0.0306, -0.0023, -0.0027,  0.0785,  0.1137, -0.1171,  0.0942,\n",
      "        -0.0296,  0.0215, -0.0844, -0.0037,  0.0345, -0.0527, -0.0367,  0.0405,\n",
      "        -0.1222,  0.0028,  0.0177,  0.0291,  0.0958,  0.0092,  0.0043, -0.1019,\n",
      "        -0.0040,  0.0837, -0.0033,  0.0011, -0.0995, -0.0936,  0.0538,  0.0418,\n",
      "         0.0936, -0.0180,  0.0274, -0.0983, -0.0168, -0.0607, -0.0861,  0.0610,\n",
      "        -0.0812, -0.0011, -0.1126, -0.0444,  0.1214,  0.0315,  0.0628, -0.0516,\n",
      "        -0.1186, -0.0298,  0.0296, -0.1093,  0.1180,  0.0667,  0.0379,  0.0640,\n",
      "         0.0083,  0.1065,  0.1197,  0.0563, -0.0192,  0.0899,  0.0264,  0.1161,\n",
      "         0.0888, -0.0479,  0.0305,  0.0413, -0.0921, -0.0365, -0.1088,  0.0331,\n",
      "        -0.0723,  0.1142,  0.0876,  0.0412,  0.0140,  0.0215, -0.1103,  0.0670,\n",
      "         0.0934, -0.0777, -0.0399,  0.0650,  0.0642,  0.0471,  0.0404, -0.0020,\n",
      "        -0.0872,  0.0249, -0.0767,  0.0694,  0.0084, -0.0889,  0.0903, -0.0447,\n",
      "        -0.0146, -0.0842,  0.0339, -0.0033, -0.0541, -0.0770,  0.0106, -0.0348,\n",
      "        -0.0232, -0.0837,  0.0736,  0.0318, -0.1126, -0.0961,  0.0201,  0.0309,\n",
      "        -0.0776, -0.1427,  0.0849,  0.0404, -0.1586,  0.1541,  0.1350, -0.1971,\n",
      "        -0.0379, -0.0683,  0.0399, -0.0907,  0.0026, -0.0273,  0.0302, -0.1199,\n",
      "        -0.0515,  0.0037,  0.1462, -0.0478, -0.0252,  0.0083, -0.0733, -0.0757,\n",
      "        -0.0722,  0.0882, -0.0778,  0.0368, -0.0575,  0.1769, -0.0490, -0.1005,\n",
      "        -0.1199, -0.0976,  0.0272,  0.0098,  0.1313, -0.1844, -0.1379, -0.0549,\n",
      "        -0.0569,  0.0446,  0.0330, -0.0764,  0.1248, -0.0699,  0.0135, -0.1074,\n",
      "         0.0712,  0.1134,  0.1008, -0.0503,  0.0800, -0.1180,  0.0324, -0.0900,\n",
      "         0.1119, -0.0265,  0.0468,  0.0896, -0.0698,  0.0961,  0.0335,  0.1122,\n",
      "        -0.0032,  0.0750,  0.1211,  0.0994, -0.1230, -0.0656,  0.1136,  0.1088,\n",
      "        -0.0090, -0.0510,  0.0321, -0.1170,  0.0286,  0.1139,  0.0233, -0.0598,\n",
      "         0.0024, -0.0529,  0.1263, -0.0784, -0.0220,  0.0615,  0.0167,  0.1182,\n",
      "        -0.0007, -0.0971,  0.0021, -0.1183, -0.0944,  0.1232, -0.0073,  0.0371,\n",
      "        -0.0758,  0.0130,  0.0687,  0.1117, -0.0975,  0.1331, -0.0313, -0.1092])), ('lstm6.bias_hh_l0', tensor([-0.1218, -0.0988,  0.1251,  0.0386,  0.0448,  0.0720, -0.0781, -0.1048,\n",
      "        -0.0644, -0.0988,  0.0975,  0.0011, -0.0794, -0.0915,  0.0805, -0.0648,\n",
      "         0.0252,  0.0525, -0.0712, -0.1173,  0.1059,  0.0893,  0.0179, -0.0742,\n",
      "        -0.0682, -0.0090, -0.0328,  0.1106,  0.0567,  0.0035, -0.0892,  0.1043,\n",
      "        -0.1011, -0.1043, -0.0118, -0.1136,  0.0567, -0.0594, -0.0798, -0.0415,\n",
      "        -0.0397,  0.0026,  0.1131, -0.1015, -0.0081, -0.1083, -0.0606, -0.1167,\n",
      "        -0.0942, -0.1009,  0.1154, -0.0819, -0.0519, -0.0936,  0.0090,  0.1098,\n",
      "        -0.0999, -0.0406,  0.1232, -0.1082,  0.0272,  0.0425,  0.0380,  0.1083,\n",
      "         0.0468, -0.0444,  0.1169, -0.0212,  0.0932,  0.1037, -0.0965, -0.0304,\n",
      "         0.0083, -0.0140,  0.0660,  0.1240,  0.1086, -0.0609,  0.0085,  0.1074,\n",
      "         0.1053,  0.0642, -0.0690,  0.0287, -0.0819, -0.0366,  0.0661, -0.0316,\n",
      "         0.0022,  0.1054, -0.0333,  0.0442, -0.0539,  0.0529, -0.0465,  0.1268,\n",
      "        -0.0309,  0.0372, -0.1027, -0.0120, -0.0422, -0.0012,  0.0475, -0.0732,\n",
      "        -0.1246, -0.1165,  0.0917,  0.1085, -0.0997, -0.1125,  0.0931,  0.0963,\n",
      "         0.0960, -0.0596,  0.0305, -0.0568, -0.1132,  0.0928, -0.0847,  0.0622,\n",
      "         0.1240, -0.0657, -0.0367,  0.1215, -0.0524,  0.0523, -0.0204, -0.0931,\n",
      "        -0.0021, -0.0518, -0.0406, -0.0991, -0.0511, -0.0950,  0.0375,  0.1648,\n",
      "         0.0315, -0.1413, -0.0303,  0.1348,  0.0330,  0.0910, -0.1260, -0.0084,\n",
      "        -0.0790,  0.1027, -0.0892,  0.1465, -0.0488,  0.1347,  0.1177, -0.0548,\n",
      "         0.1033,  0.0277, -0.0420, -0.0618,  0.0342,  0.1797,  0.0608, -0.1471,\n",
      "        -0.1718, -0.0511,  0.0901,  0.0595,  0.0180,  0.1119,  0.1805,  0.0980,\n",
      "        -0.0312, -0.0737,  0.0284, -0.0595, -0.1063,  0.1786, -0.1337,  0.0536,\n",
      "         0.0594, -0.0200, -0.0497, -0.0252, -0.1214,  0.1118, -0.1341,  0.0565,\n",
      "         0.0434,  0.0899,  0.0519,  0.1146,  0.0813, -0.1375, -0.1938,  0.0512,\n",
      "        -0.1210,  0.1034, -0.0695, -0.1183,  0.0665,  0.0611, -0.1112,  0.0090,\n",
      "         0.0235, -0.0127,  0.1029, -0.0988, -0.0787,  0.1090,  0.0184,  0.1103,\n",
      "         0.1182, -0.1218, -0.0698, -0.0812,  0.0997,  0.0474, -0.0808,  0.0363,\n",
      "         0.1046, -0.0843, -0.0810,  0.0854, -0.0812,  0.1243, -0.0849, -0.0947,\n",
      "         0.0181,  0.1131,  0.0293, -0.1038, -0.0761, -0.0011, -0.0591,  0.0940,\n",
      "        -0.0168, -0.0851,  0.0334,  0.0196,  0.0020,  0.0817,  0.1032,  0.1076,\n",
      "        -0.0152, -0.0617,  0.0349, -0.1000,  0.0758, -0.0789, -0.0901, -0.0528,\n",
      "         0.0476, -0.0843, -0.0151, -0.0003,  0.1082,  0.0932,  0.0864,  0.0568])), ('lstm7.weight_ih_l0', tensor([[-0.1526,  0.1315, -0.1646,  ..., -0.0143,  0.0690,  0.1284],\n",
      "        [ 0.0337, -0.0101,  0.1489,  ...,  0.0367,  0.0305,  0.0285],\n",
      "        [ 0.0606,  0.0959,  0.0526,  ..., -0.1759,  0.0418, -0.0317],\n",
      "        ...,\n",
      "        [ 0.0441, -0.1056, -0.0015,  ..., -0.1102, -0.0346, -0.1535],\n",
      "        [-0.0567,  0.0578, -0.0490,  ...,  0.0747, -0.0624,  0.0969],\n",
      "        [ 0.1395,  0.1620, -0.0415,  ...,  0.1252, -0.0037,  0.1396]])), ('lstm7.weight_hh_l0', tensor([[ 0.0710, -0.1014,  0.1319,  ...,  0.0706, -0.1096, -0.1555],\n",
      "        [-0.0760, -0.0221, -0.0439,  ...,  0.0492,  0.0902, -0.0270],\n",
      "        [ 0.0441,  0.1594, -0.0546,  ..., -0.1547, -0.0897,  0.0655],\n",
      "        ...,\n",
      "        [-0.1188,  0.0687, -0.0771,  ...,  0.1240,  0.1678, -0.0550],\n",
      "        [ 0.0596, -0.0522,  0.1002,  ...,  0.0980,  0.1710,  0.1506],\n",
      "        [ 0.0349,  0.1595,  0.0078,  ...,  0.0393,  0.0458, -0.1251]])), ('lstm7.bias_ih_l0', tensor([-0.0445,  0.0622,  0.2371,  0.0108,  0.1335, -0.1224, -0.0408, -0.0401,\n",
      "        -0.0770,  0.0931, -0.0379, -0.0773,  0.1645, -0.0270,  0.1101, -0.0046,\n",
      "         0.1354, -0.0556,  0.0777,  0.0639,  0.0710,  0.0987,  0.1692, -0.1044,\n",
      "         0.1377, -0.0496, -0.1331, -0.1065, -0.0585,  0.1059, -0.1182, -0.0298,\n",
      "        -0.1657,  0.0033,  0.2664,  0.0904, -0.0483, -0.1183, -0.0325,  0.1479,\n",
      "        -0.0345, -0.0176,  0.0546, -0.0675, -0.1671, -0.1557,  0.0058, -0.1696,\n",
      "        -0.1583,  0.2115,  0.1032, -0.0870,  0.0097, -0.0996, -0.0317, -0.0057,\n",
      "         0.1551,  0.1859, -0.0034,  0.0066,  0.0738,  0.1388,  0.1432, -0.0074,\n",
      "        -0.0324, -0.0743,  0.3779, -0.1522, -0.0911, -0.0555, -0.1795, -0.0445,\n",
      "        -0.2416,  0.2084,  0.0945,  0.1644, -0.0740, -0.0756, -0.1875,  0.1022,\n",
      "         0.0481, -0.1958, -0.0603, -0.0155, -0.1199,  0.0619,  0.1766, -0.1931,\n",
      "         0.3040, -0.1308, -0.1301, -0.0133, -0.0263, -0.2623, -0.2159, -0.0922,\n",
      "         0.0682, -0.0873,  0.1496,  0.0059, -0.0964,  0.0456,  0.0796,  0.0991,\n",
      "         0.0162,  0.0925, -0.1447,  0.1863,  0.0328, -0.0867,  0.1239,  0.0009,\n",
      "         0.0499, -0.1091, -0.0168,  0.0985, -0.0606,  0.1142, -0.1356, -0.1346,\n",
      "         0.0362, -0.0126,  0.0535,  0.1773,  0.0554, -0.0025, -0.0260,  0.0024])), ('lstm7.bias_hh_l0', tensor([-3.1534e-02, -1.6759e-02,  8.4093e-02, -5.3991e-02, -1.4374e-02,\n",
      "        -7.3838e-02,  1.5882e-01, -3.5185e-03,  4.8929e-02, -1.3107e-01,\n",
      "        -7.0838e-02,  1.3785e-01, -1.6257e-01, -1.1568e-01,  1.3260e-01,\n",
      "        -1.2789e-01, -2.3483e-02,  1.3201e-01,  1.1744e-01, -1.5726e-01,\n",
      "        -1.3872e-01,  1.1115e-01, -7.6154e-02,  8.1728e-02,  2.3008e-01,\n",
      "         1.8495e-01, -1.5234e-01,  1.5540e-01,  1.1865e-01,  3.9598e-02,\n",
      "        -3.1899e-03, -9.8093e-02, -6.7628e-02, -6.3706e-02,  2.8760e-01,\n",
      "         1.3624e-01,  1.6702e-01,  2.8396e-02,  2.4514e-01,  1.5674e-01,\n",
      "         1.7451e-01,  1.4862e-01, -1.6127e-01, -1.3342e-01, -1.3070e-01,\n",
      "         1.7613e-01, -1.0812e-01,  1.5640e-01, -1.5876e-01,  2.1107e-01,\n",
      "        -9.6188e-02,  6.0616e-02, -1.2932e-01,  7.3256e-02,  5.9622e-02,\n",
      "         8.7461e-02,  2.2246e-01,  5.3435e-02,  4.0008e-02,  9.0972e-02,\n",
      "         1.6864e-02, -1.0193e-01,  1.2388e-01, -2.0967e-02,  9.1898e-02,\n",
      "         8.1087e-02,  1.5871e-01, -1.2941e-01, -6.8827e-02,  1.8305e-01,\n",
      "        -3.9559e-01,  5.1750e-02, -2.4853e-01,  1.9993e-01, -1.2220e-01,\n",
      "         3.5775e-04,  4.6920e-02, -5.5561e-03, -9.2869e-02, -1.2926e-01,\n",
      "        -1.2495e-01, -2.9208e-01,  1.8458e-01, -1.7743e-01, -2.1660e-01,\n",
      "        -2.3837e-01,  1.5078e-01, -1.9515e-01,  3.0400e-01, -1.4945e-01,\n",
      "        -2.1377e-01,  2.3245e-01,  1.4317e-01, -2.6561e-01, -3.1453e-02,\n",
      "        -2.0677e-02,  1.7210e-01, -1.2569e-01,  2.9303e-01,  8.7548e-02,\n",
      "         1.2287e-01, -1.3343e-02,  2.4190e-01, -3.2152e-02,  1.0543e-01,\n",
      "        -1.3565e-01, -8.7114e-02,  1.8608e-01, -1.3732e-01,  1.4872e-02,\n",
      "        -1.0931e-01, -1.1994e-01, -3.8835e-02, -9.6887e-02, -4.2033e-03,\n",
      "        -5.3096e-02,  2.3428e-02, -4.6816e-02,  7.8326e-02,  6.0126e-02,\n",
      "         2.4322e-01,  1.6176e-01, -9.0532e-02,  1.0235e-01, -2.6719e-02,\n",
      "        -1.1570e-01,  9.4751e-02,  1.4349e-02])), ('fc.weight', tensor([[ 1.1136e-01,  3.6077e-02, -3.4286e-01,  3.5127e-02,  3.2186e-02,\n",
      "         -1.9420e-01,  1.1656e-01,  2.0373e-01,  4.0007e-02, -1.0381e-01,\n",
      "         -6.8567e-02, -1.4932e-01,  3.9195e-02,  2.7126e-02,  5.4141e-02,\n",
      "         -1.0941e-01, -1.0713e-01,  4.0459e-02, -6.3897e-02,  1.1212e-02,\n",
      "          2.3728e-01, -1.3569e-02, -1.9959e-01,  2.1582e-01, -1.9899e-01,\n",
      "          2.0754e-01, -2.6369e-02,  6.3184e-02,  1.5606e-01,  1.3973e-02,\n",
      "          1.2450e-01, -1.0050e-01],\n",
      "        [ 1.1123e-01, -1.0390e-01, -2.8039e-01, -7.1205e-02, -4.8441e-02,\n",
      "         -1.2710e-01,  1.9308e-01, -1.2288e-01, -4.4933e-02, -8.6304e-02,\n",
      "          4.6275e-02, -8.7636e-02, -1.7222e-01, -1.0542e-01,  1.5963e-01,\n",
      "          5.4249e-02,  1.8778e-02,  4.6146e-02,  1.5645e-02,  8.5138e-02,\n",
      "          9.3987e-02,  2.3287e-01, -1.5471e-01,  9.9316e-02, -1.0486e-01,\n",
      "          4.0175e-02, -7.2529e-02, -1.4848e-01, -6.2094e-02,  1.7069e-01,\n",
      "         -5.6379e-02,  2.0010e-01],\n",
      "        [ 1.1416e-01,  9.9430e-02, -2.7261e-01,  2.2078e-01, -1.4246e-01,\n",
      "          3.9312e-04,  2.4563e-01, -4.2369e-02, -1.2224e-01, -2.3822e-01,\n",
      "          7.4060e-02,  1.5576e-01, -9.0288e-02, -9.6423e-02,  1.0467e-01,\n",
      "          2.9093e-02,  5.5398e-02,  2.4162e-01, -8.3256e-03, -1.2944e-01,\n",
      "          1.3071e-01, -1.2310e-01, -3.9678e-02,  1.8475e-01, -1.4785e-01,\n",
      "         -9.9054e-02,  2.2424e-02, -9.8672e-02, -1.0818e-01,  2.6279e-03,\n",
      "         -3.2071e-02,  1.9818e-01],\n",
      "        [ 1.2407e-01,  1.6628e-01,  6.7644e-01, -2.1061e-01, -1.4648e-01,\n",
      "          5.8983e-03, -4.9953e-01, -1.1462e-01, -3.2040e-01,  1.8336e-01,\n",
      "         -2.0401e-01,  2.4295e-01, -1.3089e-01, -2.3690e-01, -2.3958e-01,\n",
      "          1.4784e-01, -1.2964e-01, -3.2610e-01,  6.9545e-02, -1.2632e-01,\n",
      "         -1.9083e-01, -3.1286e-01,  3.2104e-01, -3.0146e-01,  5.9541e-01,\n",
      "         -3.6545e-01, -2.6828e-01,  3.2054e-01,  4.5298e-03, -3.8329e-01,\n",
      "         -1.8772e-01, -1.9156e-01],\n",
      "        [-1.4679e-01,  1.3418e-01, -2.2557e-01,  1.6071e-02, -3.7827e-02,\n",
      "         -9.5524e-02,  1.7355e-01, -3.1902e-02, -1.2060e-01,  6.9408e-02,\n",
      "          5.8551e-02, -4.3332e-02,  1.1398e-01,  2.5933e-02, -7.2988e-02,\n",
      "          7.9547e-02,  1.0470e-01,  3.0096e-02, -6.0855e-02,  1.2348e-02,\n",
      "          5.9770e-02, -1.2039e-01, -1.3028e-01, -4.1860e-02, -2.4548e-01,\n",
      "          5.2192e-02,  1.4846e-01,  4.2281e-02,  6.2392e-02, -6.8030e-02,\n",
      "         -7.4519e-02,  1.3374e-02]])), ('fc.bias', tensor([-0.4548, -0.4260, -0.4197,  1.3270, -0.2740]))])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 160 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m lstm\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(weights_path))\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(lstm\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[1;32m----> 8\u001B[0m weights \u001B[38;5;241m=\u001B[39m lstm\u001B[38;5;241m.\u001B[39mfc\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     10\u001B[0m bias \u001B[38;5;241m=\u001B[39m lstm\u001B[38;5;241m.\u001B[39mfc\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (\u001B[38;5;28minput\u001B[39m, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a Tensor with 160 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accs, label='Training Accuracy')\n",
    "plt.plot(valid_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T09:31:49.277689700Z",
     "start_time": "2023-12-13T09:31:49.238416200Z"
    }
   },
   "id": "52a4803347ff7656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
