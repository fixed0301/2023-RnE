{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:14.081958600Z",
     "start_time": "2023-12-18T04:58:12.786130Z"
    }
   },
   "id": "91a24a48c60e7953"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 가상환경 GPU 사용 가능 상태\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "       device = 'cuda:0'\n",
    "       print('현재 가상환경 GPU 사용 가능 상태')\n",
    "else:\n",
    "       device = 'cpu'\n",
    "       print('GPU 사용 불가능 상태')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:17.096340700Z",
     "start_time": "2023-12-18T04:58:17.045967100Z"
    }
   },
   "id": "2a6bb59c8c16479c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 24):\n",
    "       random.seed(seed)\n",
    "       np.random.seed(seed)\n",
    "       # torch.cuda.manual_seed(seed)\n",
    "       # torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:18.717268700Z",
     "start_time": "2023-12-18T04:58:18.714268900Z"
    }
   },
   "id": "18a26b6e0c9bac97"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tot_actions = 5\n",
    "actions_name = 'backward', 'sit', 'slide', 'swing', 'walk'\n",
    "min_data_len = 30"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:20.178761300Z",
     "start_time": "2023-12-18T04:58:20.168923200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "actions_csv_dir = '../csv_1214/'\n",
    "dataset = []\n",
    "\n",
    "label_mapping = {'backward': 0,\n",
    "                 'sit': 1,\n",
    "                 'slide': 2,\n",
    "                 'swing': 3,\n",
    "                 'walk' : 4\n",
    "                 }\n",
    "\n",
    "def map_action_to_label(csv_name):\n",
    "       for action, label in label_mapping.items():\n",
    "              if action in csv_name.split('_')[0]:\n",
    "                     return label\n",
    "       return -1\n",
    "\n",
    "discard_csv = ['swing_10', 'swing_11', 'swing_12']\n",
    "\n",
    "for action_csv in os.listdir(actions_csv_dir):\n",
    "       action_df = pd.read_csv(os.path.join(actions_csv_dir, action_csv))\n",
    "       \n",
    "       label = map_action_to_label(action_csv)\n",
    "       if label != -1:\n",
    "              for idx in range(0, len(action_df), int(min_data_len / 2)):\n",
    "                     seq_df = action_df[idx: idx + min_data_len] #길이만큼 데이터 자른 것(즉 length 만큼의 프레임)\n",
    "                     if len(seq_df) == min_data_len: # 딱 length에 개수 맞춰서 끊어서 넣으려고\n",
    "                            dataset.append({'key': label, 'value': seq_df}) # key에 slide, value에는 묶음 프레임 만큼이 담기겠네\n",
    "       #최종적으로 dataset에는 행동별로 dictionary 가 만들어져 들어간다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:21.179316800Z",
     "start_time": "2023-12-18T04:58:20.898483700Z"
    }
   },
   "id": "20776e1e9089a511"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0]['value'].columns)) # z축 까지 99 (33 * 3)차원"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:36.190020800Z",
     "start_time": "2023-12-18T04:58:36.178020500Z"
    }
   },
   "id": "d656657e15c7e45f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "       def __init__(self, dataset): #모든 행동을 통합한 df가 들어가야함\n",
    "              self.x = []\n",
    "              self.y = []\n",
    "              for dic in dataset:\n",
    "                     self.y.append(dic['key']) #key 값에는 actions 들어감\n",
    "                     self.x.append(dic['value']) #action마다의 data 들어감\n",
    "\n",
    "       def __getitem__(self, index): #index는 행동의 index\n",
    "              data = self.x[index] # x에는 꺼내 쓸 (행동마다 45개 묶음프레임)의 데이터\n",
    "              label = self.y[index]\n",
    "              return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "       def __len__(self):\n",
    "              return len(self.x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:39.505261Z",
     "start_time": "2023-12-18T04:58:39.487731200Z"
    }
   },
   "id": "f6106f1224437afe"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n",
      "803, 100, 101\n"
     ]
    }
   ],
   "source": [
    "train_test_val_ratio = [0.8, 0.1, 0.1]\n",
    "print(len(dataset))\n",
    "train_len = int(len(dataset) * train_test_val_ratio[0])\n",
    "val_len = int(len(dataset) * train_test_val_ratio[1])\n",
    "test_len = len(dataset) - train_len - val_len\n",
    "print('{}, {}, {}'.format(train_len, val_len, test_len))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:46.052031Z",
     "start_time": "2023-12-18T04:58:46.042027900Z"
    }
   },
   "id": "ba330d25c6c6f524"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "CFG = {'batch_size': 32,\n",
    "       'learning_rate': 1e-3,\n",
    "       'seed':24,\n",
    "       'epochs': 100   \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:47.079384600Z",
     "start_time": "2023-12-18T04:58:47.063209600Z"
    }
   },
   "id": "14d2065c8d4dc0c6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=CFG['batch_size'])\n",
    "val_loader = DataLoader(valid_data, batch_size=CFG['batch_size'])\n",
    "test_loader = DataLoader(test_data, batch_size=CFG['batch_size'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:47.982642500Z",
     "start_time": "2023-12-18T04:58:47.966642200Z"
    }
   },
   "id": "9fa7ab00e37041cc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model1, self).__init__()\n",
    "              self.lstm1 = nn.LSTM(input_size=75, hidden_size=128, num_layers=1, batch_first=True) #input은  45 * 3(x, y z)\n",
    "              self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n",
    "              self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)\n",
    "              self.dropout1 = nn.Dropout(0.1)\n",
    "              self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
    "              self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n",
    "              self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "              self.dropout2 = nn.Dropout(0.1)\n",
    "              self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)\n",
    "              self.fc = nn.Linear(32, 5) #분류할 클래스 5가지\n",
    "\n",
    "       def forward(self, x):\n",
    "              x, _ = self.lstm1(x)\n",
    "              x, _ = self.lstm2(x)\n",
    "              x, _ = self.lstm3(x)\n",
    "              x = self.dropout1(x)\n",
    "              x, _ = self.lstm4(x)\n",
    "              x, _ = self.lstm5(x)\n",
    "              x, _ = self.lstm6(x)\n",
    "              x = self.dropout2(x)\n",
    "              x, _ = self.lstm7(x)\n",
    "              x = self.fc(x[:, -1, :])\n",
    "              return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:49.209374700Z",
     "start_time": "2023-12-18T04:58:49.193863700Z"
    }
   },
   "id": "d712906be6495443"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Model 2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "       def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "              super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "              assert hidden_channels % 2 == 0\n",
    "\n",
    "              self.input_channels = input_channels\n",
    "              self.hidden_channels = hidden_channels\n",
    "              self.kernel_size = kernel_size\n",
    "              self.num_features = 5\n",
    "\n",
    "              self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "              self.Wxi = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whi = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxf = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whf = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxc = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Whc = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "              self.Wxo = nn.Conv1d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "              self.Who = nn.Conv1d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "              self.Wci = None\n",
    "              self.Wcf = None\n",
    "              self.Wco = None\n",
    "\n",
    "       def forward(self, x, h, c):\n",
    "              ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "              cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "              cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "              co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "              ch = co * torch.tanh(cc)\n",
    "              return ch\n",
    "\n",
    "       def init_hidden(self, batch_size, hidden, shape):\n",
    "              if self.Wci is None:\n",
    "                     self.Wci = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "                     self.Wcf = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "                     self.Wco = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1]))\n",
    "              else:\n",
    "                     assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "                     assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "              return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])),\n",
    "                      Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])))\n",
    "\n",
    "class Model2(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model2, self).__init__()\n",
    "              self.conv_lstm1 = ConvLSTMCell(input_channels=75, hidden_channels=128, kernel_size=5)\n",
    "              self.conv_lstm2 = ConvLSTMCell(input_channels=128, hidden_channels=256, kernel_size=5)\n",
    "\n",
    "              self.dropout1 = nn.Dropout(0.1)\n",
    "              \n",
    "              self.conv_lstm4 = ConvLSTMCell(input_channels=256, hidden_channels=128, kernel_size=5)\n",
    "              self.conv_lstm5 = ConvLSTMCell(input_channels=128, hidden_channels=64, kernel_size=5)\n",
    "              \n",
    "              self.dropout2 = nn.Dropout(0.1)\n",
    "              self.conv_lstm6 = ConvLSTMCell(input_channels=64, hidden_channels=32, kernel_size=5)\n",
    "\n",
    "              self.fc = nn.Linear(32, 5)\n",
    "              self.step = 10  \n",
    "              self.num_layers = 6  \n",
    "              self.effective_step = [9]\n",
    "           \n",
    "       def forward(self, x):\n",
    "              internal_state = []\n",
    "              outputs = []\n",
    "\n",
    "              for step in range(self.step):\n",
    "                     for i in range(self.num_layers):\n",
    "                            name = 'conv_lstm{}'.format(i + 1)\n",
    "\n",
    "                            if step == 0:\n",
    "                                   bsize, _, height, width = x.size()\n",
    "                                   (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i], shape=(height, width))\n",
    "                                   internal_state.append((h, c))\n",
    "\n",
    "                            (h, c) = internal_state[i]\n",
    "                            x, new_c = getattr(self, name)(x, h, c)\n",
    "                            internal_state[i] = (x, new_c)\n",
    "\n",
    "                     if step in self.effective_step:\n",
    "                            outputs.append(x)\n",
    "\n",
    "              last_hidden_state = internal_state[-1][0]\n",
    "              \n",
    "              output = self.fc(last_hidden_state[:, -1, :])\n",
    "\n",
    "              return output\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:50.036725900Z",
     "start_time": "2023-12-18T04:58:50.023761200Z"
    }
   },
   "id": "a8e56a0b61128b5d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Model3(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model3, self).__init__()\n",
    "\n",
    "              self.conv = nn.Conv1d(30, 36, 7, 1)\n",
    "              self.nonLin = nn.BatchNorm1d(30)\n",
    "\n",
    "              self.recurrent_layer = nn.LSTM(hidden_size=100, input_size=75, bidirectional=True)\n",
    "\n",
    "              self.recurrent_layer2 = nn.LSTM(hidden_size=100, input_size=200, bidirectional=True)\n",
    "\n",
    "              self.nonLin2 = nn.BatchNorm1d(200)\n",
    "              self.activation = nn.LeakyReLU(0.2)  # Leaky ReLU activation\n",
    "              self.dropout = nn.Dropout(0.5)\n",
    "              self.classify_layer = nn.Linear(200 * 2, 5)\n",
    "\n",
    "              # Weight initialization\n",
    "              init.xavier_uniform_(self.conv.weight)\n",
    "              init.xavier_uniform_(self.classify_layer.weight)\n",
    "\n",
    "       def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "              rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
    "              lin1 = self.nonLin(rnn_outputs)\n",
    "\n",
    "              rnn_outputs2, (hn2, cn2) = self.recurrent_layer2(lin1)\n",
    "\n",
    "              lin2 = self.nonLin2(rnn_outputs2)\n",
    "              conv = self.conv(lin2)\n",
    "              activation = self.activation(conv)\n",
    "              activation = activation.permute(0, 2, 1)\n",
    "              activation = self.dropout(activation)\n",
    "\n",
    "              flatten = activation.view(activation.size(0), -1)\n",
    "\n",
    "              logits = self.classify_layer(flatten)\n",
    "              return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:50.782343800Z",
     "start_time": "2023-12-18T04:58:50.771341500Z"
    }
   },
   "id": "a40e1fb638676a05"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Model4(nn.Module):\n",
    "       def __init__(self):\n",
    "              super(Model4, self).__init__()\n",
    "\n",
    "              self.conv = nn.Conv1d(30, 36, 7, 1)\n",
    "              self.nonLin = nn.BatchNorm1d(30)\n",
    "\n",
    "              self.recurrent_layer = nn.GRU(hidden_size=100, input_size=75, bidirectional=True)\n",
    "\n",
    "              self.recurrent_layer2 = nn.GRU(hidden_size=200, input_size=100, bidirectional=True)\n",
    "\n",
    "              self.nonLin2 = nn.BatchNorm1d(200)\n",
    "              self.activation = nn.LeakyReLU(0.2)  # Leaky ReLU activation\n",
    "              self.dropout = nn.Dropout(0.5)\n",
    "              self.classify_layer = nn.Linear(200 * 2, 5)\n",
    "\n",
    "              # Weight initialization\n",
    "              init.xavier_uniform_(self.conv.weight)\n",
    "              init.xavier_uniform_(self.classify_layer.weight)\n",
    "\n",
    "       def forward(self, input, h_t_1=None, c_t_1=None):\n",
    "              rnn_outputs, (hn, cn) = self.recurrent_layer(input)\n",
    "              lin1 = self.nonLin(rnn_outputs)\n",
    "\n",
    "              rnn_outputs2, (hn2, cn2) = self.recurrent_layer2(lin1)\n",
    "\n",
    "              lin2 = self.nonLin2(rnn_outputs2)\n",
    "              conv = self.conv(lin2)\n",
    "              activation = self.activation(conv)\n",
    "              activation = activation.permute(0, 2, 1)\n",
    "              activation = self.dropout(activation)\n",
    "\n",
    "              flatten = activation.view(activation.size(0), -1)\n",
    "\n",
    "              logits = self.classify_layer(flatten)\n",
    "              return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T04:58:51.496222700Z",
     "start_time": "2023-12-18T04:58:51.483992600Z"
    }
   },
   "id": "f43dbc96489edcbc"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.66451 valid loss: 4.09086 lr: 0.00100 \n",
      "Validation loss decreased (inf --> 4.090860).  Saving model ...\n",
      "Validation loss decreased (4.090860 --> 3.964100).  Saving model ...\n",
      "Validation loss decreased (3.964100 --> 3.852710).  Saving model ...\n",
      "Validation loss decreased (3.852710 --> 3.756910).  Saving model ...\n",
      "Validation loss decreased (3.756910 --> 3.674700).  Saving model ...\n",
      "Validation loss decreased (3.674700 --> 3.604240).  Saving model ...\n",
      "Validation loss decreased (3.604240 --> 3.544010).  Saving model ...\n",
      "Validation loss decreased (3.544010 --> 3.492640).  Saving model ...\n",
      "Validation loss decreased (3.492640 --> 3.448920).  Saving model ...\n",
      "Validation loss decreased (3.448920 --> 3.411840).  Saving model ...\n",
      "Epoch: 10, loss: 1.36830 valid loss: 3.38043 lr: 0.00100 \n",
      "Validation loss decreased (3.411840 --> 3.380430).  Saving model ...\n",
      "Validation loss decreased (3.380430 --> 3.353870).  Saving model ...\n",
      "Validation loss decreased (3.353870 --> 3.331410).  Saving model ...\n",
      "Validation loss decreased (3.331410 --> 3.312380).  Saving model ...\n",
      "Validation loss decreased (3.312380 --> 3.296240).  Saving model ...\n",
      "Validation loss decreased (3.296240 --> 3.282500).  Saving model ...\n",
      "Validation loss decreased (3.282500 --> 3.270760).  Saving model ...\n",
      "Validation loss decreased (3.270760 --> 3.260680).  Saving model ...\n",
      "Validation loss decreased (3.260680 --> 3.252000).  Saving model ...\n",
      "Validation loss decreased (3.252000 --> 3.244480).  Saving model ...\n",
      "Epoch: 20, loss: 1.30902 valid loss: 3.23793 lr: 0.00100 \n",
      "Validation loss decreased (3.244480 --> 3.237930).  Saving model ...\n",
      "Validation loss decreased (3.237930 --> 3.232210).  Saving model ...\n",
      "Validation loss decreased (3.232210 --> 3.227170).  Saving model ...\n",
      "Validation loss decreased (3.227170 --> 3.222740).  Saving model ...\n",
      "Validation loss decreased (3.222740 --> 3.218800).  Saving model ...\n",
      "Validation loss decreased (3.218800 --> 3.215310).  Saving model ...\n",
      "Validation loss decreased (3.215310 --> 3.212200).  Saving model ...\n",
      "Validation loss decreased (3.212200 --> 3.209430).  Saving model ...\n",
      "Validation loss decreased (3.209430 --> 3.206960).  Saving model ...\n",
      "Validation loss decreased (3.206960 --> 3.204740).  Saving model ...\n",
      "Epoch: 30, loss: 1.29438 valid loss: 3.20276 lr: 0.00100 \n",
      "Validation loss decreased (3.204740 --> 3.202760).  Saving model ...\n",
      "Validation loss decreased (3.202760 --> 3.201000).  Saving model ...\n",
      "Validation loss decreased (3.201000 --> 3.199420).  Saving model ...\n",
      "Validation loss decreased (3.199420 --> 3.198020).  Saving model ...\n",
      "Validation loss decreased (3.198020 --> 3.196770).  Saving model ...\n",
      "Validation loss decreased (3.196770 --> 3.195660).  Saving model ...\n",
      "Validation loss decreased (3.195660 --> 3.194670).  Saving model ...\n",
      "Validation loss decreased (3.194670 --> 3.193790).  Saving model ...\n",
      "Validation loss decreased (3.193790 --> 3.193020).  Saving model ...\n",
      "Validation loss decreased (3.193020 --> 3.192340).  Saving model ...\n",
      "Epoch: 40, loss: 1.28882 valid loss: 3.19175 lr: 0.00100 \n",
      "Validation loss decreased (3.192340 --> 3.191750).  Saving model ...\n",
      "Validation loss decreased (3.191750 --> 3.191230).  Saving model ...\n",
      "Validation loss decreased (3.191230 --> 3.190780).  Saving model ...\n",
      "Validation loss decreased (3.190780 --> 3.190390).  Saving model ...\n",
      "Validation loss decreased (3.190390 --> 3.190050).  Saving model ...\n",
      "Validation loss decreased (3.190050 --> 3.189770).  Saving model ...\n",
      "Validation loss decreased (3.189770 --> 3.189540).  Saving model ...\n",
      "Validation loss decreased (3.189540 --> 3.189340).  Saving model ...\n",
      "Validation loss decreased (3.189340 --> 3.189180).  Saving model ...\n",
      "Validation loss decreased (3.189180 --> 3.189060).  Saving model ...\n",
      "Epoch: 50, loss: 1.28660 valid loss: 3.18897 lr: 0.00100 \n",
      "Validation loss decreased (3.189060 --> 3.188970).  Saving model ...\n",
      "Validation loss decreased (3.188970 --> 3.188910).  Saving model ...\n",
      "Validation loss decreased (3.188910 --> 3.188870).  Saving model ...\n",
      "Validation loss decreased (3.188870 --> 3.188850).  Saving model ...\n",
      "Validation loss decreased (3.188850 --> 3.188850).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 30\n",
      "EarlyStopping counter: 2 out of 30\n",
      "EarlyStopping counter: 3 out of 30\n",
      "EarlyStopping counter: 4 out of 30\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 60, loss: 1.28515 valid loss: 3.18919 lr: 0.00100 \n",
      "EarlyStopping counter: 6 out of 30\n",
      "EarlyStopping counter: 7 out of 30\n",
      "EarlyStopping counter: 8 out of 30\n",
      "EarlyStopping counter: 9 out of 30\n",
      "EarlyStopping counter: 10 out of 30\n",
      "EarlyStopping counter: 11 out of 30\n",
      "EarlyStopping counter: 12 out of 30\n",
      "EarlyStopping counter: 13 out of 30\n",
      "EarlyStopping counter: 14 out of 30\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 70, loss: 1.28444 valid loss: 3.19038 lr: 0.00100 \n",
      "EarlyStopping counter: 16 out of 30\n",
      "EarlyStopping counter: 17 out of 30\n",
      "EarlyStopping counter: 18 out of 30\n",
      "EarlyStopping counter: 19 out of 30\n",
      "EarlyStopping counter: 20 out of 30\n",
      "EarlyStopping counter: 21 out of 30\n",
      "EarlyStopping counter: 22 out of 30\n",
      "EarlyStopping counter: 23 out of 30\n",
      "EarlyStopping counter: 24 out of 30\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 80, loss: 1.28412 valid loss: 3.19178 lr: 0.00100 \n",
      "EarlyStopping counter: 26 out of 30\n",
      "EarlyStopping counter: 27 out of 30\n",
      "EarlyStopping counter: 28 out of 30\n",
      "EarlyStopping counter: 29 out of 30\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 84, loss: 1.28437 valid loss: 3.19232 lr: 0.00100 \n"
     ]
    }
   ],
   "source": [
    "from dev.pytorchtools import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=30, verbose=True)\n",
    "\n",
    "num_epochs = CFG['epochs']\n",
    "learning_rate = CFG['learning_rate']\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "val_acc_max = 0.75\n",
    "val_loss_min = float('inf')  # 초기값을 양의 무한대로 설정\n",
    "\n",
    "lstm = Model1()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=60,\n",
    "                                                       factor=0.1, min_lr=1e-6, eps=1e-08)\n",
    "\n",
    "for epoch in range(num_epochs + 1):\n",
    "       losses = []\n",
    "       acc_list = []\n",
    "       lstm.train()\n",
    "       for i, (input, target) in enumerate(train_loader):\n",
    "              optimizer.zero_grad()\n",
    "              outputs = lstm(input)\n",
    "              loss = criterion(outputs, target.long())\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "              losses.append(loss.item())\n",
    "\n",
    "              preds = torch.argmax(outputs, dim=1)\n",
    "              batch_acc = (preds == target).float().mean()\n",
    "              acc_list.append(batch_acc.item())\n",
    "\n",
    "       train_losses.append(np.mean(np.array(losses)))\n",
    "       train_accs.append(np.mean(np.array(acc_list)))\n",
    "\n",
    "       losses = []\n",
    "       acc_list = []\n",
    "\n",
    "       lstm.eval()\n",
    "       with torch.no_grad():\n",
    "              correct = 0\n",
    "              total = 0\n",
    "              val_loss = 0\n",
    "              for i, (input, target) in enumerate(val_loader):\n",
    "                     outputs = lstm(input)\n",
    "                     _, predicted = torch.max(outputs.data, 1)\n",
    "                     target = target.long()\n",
    "                     val_loss += criterion(outputs, target)\n",
    "                     losses.append(val_loss.item())\n",
    "\n",
    "                     total += target.size(0)\n",
    "                     correct += (predicted == target).sum()\n",
    "\n",
    "              accuracy = 100 * correct / total\n",
    "              valid_losses.append(np.mean(np.array(losses)))\n",
    "              valid_accs.append(accuracy.item())\n",
    "\n",
    "              if epoch % 10 == 0:\n",
    "                     print(\"Epoch: %d, loss: %1.5f valid loss: %1.5f lr: %1.5f \" %\n",
    "                           (epoch, train_losses[-1], valid_losses[-1], optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "              if val_loss_min > valid_losses[-1]:  \n",
    "                     val_loss_min = valid_losses[-1]\n",
    "                     torch.save(lstm, fr'C:\\Users\\USER\\Desktop\\19rne\\2023-RnE-main\\save_by_loss\\model_.pth')\n",
    "\n",
    "              scheduler.step(valid_losses[-1])\n",
    "\n",
    "              early_stopping(round(valid_losses[-1], 5), lstm)\n",
    "\n",
    "              if early_stopping.early_stop:\n",
    "                     print(\"Epoch: %d, loss: %1.5f valid loss: %1.5f lr: %1.5f \" %\n",
    "                           (epoch, train_losses[-1], valid_losses[-1], optimizer.param_groups[0][\"lr\"]))\n",
    "                     break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T05:00:59.337578300Z",
     "start_time": "2023-12-18T04:58:56.668728200Z"
    }
   },
   "id": "21dbba28f07a11d8"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses, valid_losses, train_accs, valid_accs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T05:01:20.714593200Z",
     "start_time": "2023-12-18T05:01:20.696929800Z"
    }
   },
   "id": "d93d180c6102c1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1752\\459484574.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1752\\459484574.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1752\\459484574.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1752\\459484574.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_1752\\459484574.py:1: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m res \u001B[38;5;241m=\u001B[39m [\u001B[43m[\u001B[49m\u001B[38;5;241;43m1.6645072927841773\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.6160603761672974\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.5690331825843225\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.5281135990069463\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.4931477308273315\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.4632783302894006\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.437547967984126\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.4158186545738807\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3972229636632478\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.381659759924962\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3682966690797072\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3570995514209454\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3477443548349233\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3398027007396405\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.332908048079564\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3272519340881934\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.322229816363408\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3183267712593079\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3146007565351634\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3116262142474835\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.309016580765064\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3065645419634306\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3046945287631109\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3028130072813768\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3014338337458098\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.299690374961266\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.298569454596593\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2972084375528188\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.296171206694383\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2949942854734569\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2943815497251658\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.293615698814392\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.292873345888578\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2920041175989003\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2915958853868337\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2907948264708886\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2904181021910448\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.289965244439932\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.289464024397043\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2891363226450407\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2888218210293696\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2884404796820421\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2882084158750682\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2879399886498084\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2874946089891286\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2874214695050166\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2871180039185743\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2869252195725074\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2867666941422682\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2865648819850042\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2866040559915395\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2863171054766729\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2863825605465815\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2859925719407888\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2858293469135578\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2857366937857408\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2858472237220178\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2856693680469806\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2851948967346778\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2853522392419667\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2851463372890766\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2851203404940093\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2852068084936876\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2850808409544139\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2850570174363942\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2848643935643709\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.284759636108692\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2847790534679706\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.284824325488164\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2847741108674269\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2844424018493066\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2845483834926898\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2846238315105438\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2843253772992353\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2843242448109846\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2842233777046204\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2845360132364125\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2841830941346974\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2845137119293213\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2843562914774969\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2841233679881463\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2842514285674462\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.284220509804212\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2841655543217292\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.2843667199978461\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4.090862214565277\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.964098572731018\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.852714091539383\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.7569140791893005\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.6746982038021088\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.6042402386665344\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.544006198644638\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.4926357865333557\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.448920786380768\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.4118373692035675\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.3804314732551575\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.353868752717972\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.3314106166362762\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.3123838007450104\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.296239912509918\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.282495230436325\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2707569301128387\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.260680377483368\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2520035803318024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2444784939289093\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2379291355609894\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.232205718755722\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2271696627140045\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.222735345363617\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2187981605529785\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.215308129787445\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.212197184562683\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2094317972660065\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.206957906484604\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.204738438129425\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.2027608454227448\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.200996845960617\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1994248926639557\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.198022186756134\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1967703104019165\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1956611573696136\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1946721971035004\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1937944293022156\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.193017363548279\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1923393607139587\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1917509138584137\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1912316977977753\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1907759308815002\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.19038850069046\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.190050482749939\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189771205186844\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189537525177002\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189339339733124\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1891814172267914\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1890613734722137\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.18897345662117\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1889072358608246\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1888664066791534\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1888466477394104\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1888527274131775\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1888833343982697\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1889193058013916\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.188969135284424\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189032018184662\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189104199409485\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1891914308071136\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189284026622772\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1893858313560486\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1894978880882263\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1896104216575623\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.189728409051895\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1898582577705383\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1899873316287994\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1901169419288635\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.190248131752014\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1903818249702454\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1905190646648407\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1906570196151733\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1908005475997925\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1909464299678802\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1910910606384277\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.191228300333023\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.191367268562317\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1915021538734436\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.191645920276642\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1917780339717865\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1919074058532715\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1920455992221832\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.192184418439865\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.1923215091228485\u001B[39;49m\u001B[43m]\u001B[49m [\u001B[38;5;241m0.03485576923076923\u001B[39m, \u001B[38;5;241m0.03485576923076923\u001B[39m, \u001B[38;5;241m0.43589743627951694\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m, \u001B[38;5;241m0.5080128208949015\u001B[39m] [\u001B[38;5;241m4.0\u001B[39m, \u001B[38;5;241m4.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m, \u001B[38;5;241m52.0\u001B[39m]]\n",
      "\u001B[1;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "res = [[1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461] [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485] [0.03485576923076923, 0.03485576923076923, 0.43589743627951694, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015, 0.5080128208949015] [4.0, 4.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T05:03:38.110320200Z",
     "start_time": "2023-12-18T05:03:38.073065900Z"
    }
   },
   "id": "5040db817e8f12f5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('lstm1.weight_ih_l0', tensor([[ 0.0106,  0.0242, -0.0078,  ...,  0.0666, -0.0148, -0.0841],\n",
      "        [-0.0231, -0.0550,  0.0528,  ...,  0.0713, -0.0040, -0.0330],\n",
      "        [ 0.0231,  0.0690,  0.0860,  ...,  0.0452,  0.0325,  0.0387],\n",
      "        ...,\n",
      "        [ 0.0411, -0.0150,  0.0160,  ...,  0.0471, -0.0437, -0.0269],\n",
      "        [ 0.0147, -0.0360, -0.0769,  ..., -0.0176, -0.0588, -0.0270],\n",
      "        [-0.0545,  0.0651,  0.0564,  ..., -0.0187,  0.0805, -0.0175]])), ('lstm1.weight_hh_l0', tensor([[ 0.0696, -0.0242,  0.0037,  ...,  0.0748, -0.0340,  0.0232],\n",
      "        [-0.0830,  0.0406, -0.0691,  ..., -0.0431, -0.0609, -0.0500],\n",
      "        [ 0.0299,  0.0226,  0.0513,  ...,  0.0128,  0.0134,  0.0247],\n",
      "        ...,\n",
      "        [-0.0672,  0.0084, -0.0638,  ..., -0.0733, -0.0641, -0.0737],\n",
      "        [ 0.0594,  0.0357,  0.0599,  ...,  0.0397, -0.0346, -0.0428],\n",
      "        [-0.0431,  0.0424, -0.0633,  ..., -0.0643,  0.0663,  0.0109]])), ('lstm1.bias_ih_l0', tensor([-0.0511, -0.0311, -0.0554,  0.0446,  0.0198,  0.0584, -0.0338,  0.0323,\n",
      "         0.0234, -0.0179,  0.0301,  0.0252, -0.0205, -0.0128, -0.0665, -0.0387,\n",
      "         0.0821, -0.0238,  0.0094, -0.0196, -0.0577,  0.0713,  0.0605, -0.0628,\n",
      "         0.0839,  0.0688,  0.0036,  0.0668,  0.0339,  0.0798,  0.0400,  0.0373,\n",
      "        -0.0638,  0.0360,  0.0832, -0.0033, -0.0445, -0.0572, -0.0005,  0.0089,\n",
      "        -0.0321,  0.0092, -0.0852, -0.0431, -0.0259,  0.0853,  0.0068, -0.0140,\n",
      "        -0.0008, -0.0415, -0.0811, -0.0454,  0.0833, -0.0664,  0.0190,  0.0169,\n",
      "         0.0460,  0.0584,  0.0322,  0.0822, -0.0080, -0.0122, -0.0322, -0.0659,\n",
      "         0.0843, -0.0072,  0.0047, -0.0860,  0.0726,  0.0479, -0.0055, -0.0010,\n",
      "         0.0311,  0.0812,  0.0264,  0.0446,  0.0503, -0.0380,  0.0647, -0.0573,\n",
      "         0.0876,  0.0543, -0.0776,  0.0011, -0.0758,  0.0667,  0.0683, -0.0582,\n",
      "         0.0731,  0.0621, -0.0375,  0.0455,  0.0045,  0.0872, -0.0873, -0.0085,\n",
      "         0.0670, -0.0802,  0.0701, -0.0452,  0.0049,  0.0629, -0.0007,  0.0677,\n",
      "         0.0620,  0.0244, -0.0835, -0.0668,  0.0257,  0.0587,  0.0329, -0.0868,\n",
      "        -0.0227,  0.0833, -0.0031, -0.0855, -0.0060,  0.0295,  0.0721, -0.0455,\n",
      "        -0.0099,  0.0556, -0.0407, -0.0584, -0.0701,  0.0511, -0.0266, -0.0211,\n",
      "        -0.0602, -0.0665, -0.0027, -0.0695, -0.0812, -0.0532, -0.0250,  0.0224,\n",
      "         0.0628, -0.0716,  0.0155, -0.0462,  0.0394, -0.0427,  0.0620,  0.0321,\n",
      "        -0.0167,  0.0031, -0.0615,  0.0650, -0.0794,  0.0601, -0.0363,  0.0877,\n",
      "         0.0235,  0.0786,  0.0032,  0.0858, -0.0696, -0.0049, -0.0136,  0.0367,\n",
      "         0.0755,  0.0756, -0.0705, -0.0431, -0.0190, -0.0067, -0.0350,  0.0454,\n",
      "        -0.0678, -0.0865,  0.0759, -0.0464, -0.0183,  0.0610, -0.0177,  0.0144,\n",
      "         0.0663,  0.0463,  0.0228,  0.0049,  0.0491,  0.0144,  0.0555, -0.0440,\n",
      "        -0.0513,  0.0466,  0.0558,  0.0030, -0.0727,  0.0234,  0.0389,  0.0335,\n",
      "        -0.0655, -0.0163,  0.0397,  0.0283, -0.0509,  0.0067,  0.0263,  0.0833,\n",
      "         0.0514, -0.0606, -0.0016, -0.0757,  0.0138,  0.0357, -0.0312,  0.0271,\n",
      "         0.0164, -0.0584,  0.0003,  0.0555, -0.0375, -0.0271,  0.0416,  0.0767,\n",
      "        -0.0760,  0.0418, -0.0367, -0.0168,  0.0251,  0.0124, -0.0867, -0.0056,\n",
      "         0.0873,  0.0631,  0.0424, -0.0431,  0.0123,  0.0673,  0.0675,  0.0094,\n",
      "         0.0004, -0.0550, -0.0036,  0.0875, -0.0870,  0.0387,  0.0683,  0.0879,\n",
      "         0.0407,  0.0675, -0.0752, -0.0738,  0.0352, -0.0606,  0.0244,  0.0865,\n",
      "         0.0685,  0.0756,  0.0755, -0.0234,  0.0018,  0.0534, -0.0549,  0.0038,\n",
      "        -0.0150, -0.0371, -0.0017,  0.0835, -0.0740,  0.0296,  0.0559,  0.0757,\n",
      "         0.0399,  0.0171,  0.0430, -0.0810, -0.0266, -0.0598,  0.0279,  0.0881,\n",
      "        -0.0093,  0.0353, -0.0228,  0.0727,  0.0581, -0.0460,  0.0715,  0.0028,\n",
      "         0.0450, -0.0038,  0.0505, -0.0202, -0.0293, -0.0328, -0.0733, -0.0077,\n",
      "        -0.0674,  0.0587,  0.0470,  0.0321, -0.0692, -0.0812, -0.0260,  0.0830,\n",
      "         0.0830, -0.0098,  0.0446,  0.0044,  0.0521, -0.0786, -0.0013, -0.0466,\n",
      "         0.0240,  0.0340, -0.0408,  0.0221, -0.0012, -0.0557,  0.0814,  0.0489,\n",
      "        -0.0104, -0.0767, -0.0107,  0.0367,  0.0752, -0.0586,  0.0261, -0.0701,\n",
      "        -0.0588,  0.0670, -0.0482, -0.0226, -0.0878,  0.0762, -0.0433, -0.0387,\n",
      "        -0.0215, -0.0131,  0.0819,  0.0084, -0.0278,  0.0361, -0.0730,  0.0602,\n",
      "         0.0274,  0.0871, -0.0575, -0.0178,  0.0336, -0.0710,  0.0847, -0.0531,\n",
      "        -0.0754,  0.0093, -0.0497,  0.0088, -0.0230,  0.0524, -0.0646,  0.0550,\n",
      "        -0.0050, -0.0783,  0.0456, -0.0240, -0.0393, -0.0135, -0.0165, -0.0690,\n",
      "        -0.0237,  0.0213, -0.0201, -0.0817, -0.0831, -0.0578,  0.0675, -0.0323,\n",
      "         0.0832,  0.0063,  0.0087, -0.0795, -0.0681, -0.0204,  0.0853,  0.0058,\n",
      "        -0.0830,  0.0487,  0.0391, -0.0275, -0.0040, -0.0810, -0.0737, -0.0262,\n",
      "        -0.0216,  0.0301, -0.0786,  0.0272, -0.0181,  0.0230, -0.0781,  0.0755,\n",
      "        -0.0681, -0.0413, -0.0622, -0.0845, -0.0304, -0.0764, -0.0542, -0.0794,\n",
      "        -0.0414, -0.0347, -0.0184, -0.0825, -0.0452, -0.0002,  0.0367,  0.0252,\n",
      "         0.0381,  0.0278,  0.0050,  0.0775, -0.0683, -0.0736, -0.0517, -0.0547,\n",
      "         0.0029, -0.0528,  0.0748,  0.0150, -0.0502, -0.0641,  0.0549,  0.0771,\n",
      "         0.0866, -0.0543, -0.0766, -0.0599, -0.0381, -0.0155, -0.0605, -0.0346,\n",
      "        -0.0713,  0.0867, -0.0094, -0.0560,  0.0312,  0.0341,  0.0830, -0.0552,\n",
      "        -0.0183,  0.0816,  0.0780,  0.0525, -0.0457, -0.0559,  0.0052,  0.0082,\n",
      "        -0.0138, -0.0348, -0.0596, -0.0663,  0.0808,  0.0511,  0.0197, -0.0789,\n",
      "        -0.0293,  0.0778,  0.0164, -0.0789,  0.0440,  0.0229,  0.0509,  0.0130,\n",
      "        -0.0594,  0.0073,  0.0460,  0.0293, -0.0059,  0.0092,  0.0399, -0.0563,\n",
      "        -0.0872, -0.0853,  0.0646, -0.0496,  0.0088,  0.0651,  0.0599, -0.0344,\n",
      "        -0.0730, -0.0031,  0.0829,  0.0105, -0.0278, -0.0580, -0.0031,  0.0019,\n",
      "        -0.0024,  0.0572, -0.0094,  0.0059,  0.0132,  0.0209,  0.0118,  0.0101,\n",
      "         0.0591,  0.0347, -0.0393,  0.0694,  0.0449,  0.0645, -0.0804,  0.0857,\n",
      "         0.0040,  0.0661, -0.0614, -0.0095, -0.0575, -0.0616,  0.0548, -0.0640])), ('lstm1.bias_hh_l0', tensor([-0.0474, -0.0371,  0.0881,  0.0340, -0.0575,  0.0009, -0.0239, -0.0294,\n",
      "         0.0447, -0.0761,  0.0123,  0.0480, -0.0062, -0.0536,  0.0132, -0.0093,\n",
      "         0.0059, -0.0150,  0.0142, -0.0054,  0.0071, -0.0728, -0.0614, -0.0869,\n",
      "         0.0561,  0.0517, -0.0394,  0.0468, -0.0164,  0.0384, -0.0548, -0.0199,\n",
      "        -0.0183,  0.0416, -0.0506, -0.0724,  0.0666,  0.0486,  0.0036, -0.0562,\n",
      "         0.0087, -0.0529,  0.0381,  0.0851,  0.0229, -0.0865, -0.0702,  0.0006,\n",
      "         0.0778, -0.0232, -0.0853, -0.0503, -0.0592,  0.0039,  0.0310, -0.0130,\n",
      "         0.0078, -0.0519, -0.0845, -0.0014, -0.0511, -0.0057, -0.0064,  0.0607,\n",
      "        -0.0341,  0.0678, -0.0502, -0.0058, -0.0455, -0.0176,  0.0565,  0.0725,\n",
      "        -0.0256,  0.0466,  0.0368,  0.0073, -0.0651, -0.0475, -0.0524,  0.0626,\n",
      "         0.0425,  0.0372,  0.0415, -0.0160,  0.0019, -0.0228, -0.0492, -0.0868,\n",
      "        -0.0067,  0.0562, -0.0685, -0.0880,  0.0090, -0.0583,  0.0584, -0.0106,\n",
      "         0.0640, -0.0394, -0.0840,  0.0477, -0.0363,  0.0505, -0.0684,  0.0529,\n",
      "        -0.0241,  0.0639,  0.0169, -0.0141, -0.0224,  0.0190,  0.0766, -0.0152,\n",
      "         0.0884,  0.0156,  0.0011, -0.0345, -0.0738,  0.0038,  0.0699, -0.0164,\n",
      "         0.0128, -0.0305, -0.0283, -0.0844, -0.0017,  0.0617, -0.0383, -0.0329,\n",
      "         0.0137,  0.0604, -0.0373,  0.0565,  0.0344, -0.0330,  0.0609,  0.0275,\n",
      "         0.0344, -0.0751, -0.0224,  0.0562, -0.0760,  0.0075,  0.0557,  0.0265,\n",
      "         0.0500, -0.0111,  0.0170,  0.0349,  0.0014,  0.0088,  0.0372,  0.0325,\n",
      "        -0.0522,  0.0371,  0.0011, -0.0380,  0.0608, -0.0446, -0.0208,  0.0215,\n",
      "         0.0621,  0.0395,  0.0696,  0.0878, -0.0675, -0.0731,  0.0590,  0.0705,\n",
      "        -0.0470,  0.0045, -0.0624,  0.0278,  0.0346,  0.0150, -0.0120,  0.0627,\n",
      "         0.0844,  0.0883,  0.0351,  0.0353, -0.0280, -0.0536,  0.0007,  0.0819,\n",
      "         0.0683, -0.0083, -0.0260, -0.0526,  0.0771, -0.0727,  0.0125,  0.0674,\n",
      "        -0.0581, -0.0846, -0.0607,  0.0565,  0.0002,  0.0597, -0.0206,  0.0130,\n",
      "         0.0118, -0.0652,  0.0848, -0.0455, -0.0691,  0.0087,  0.0480,  0.0733,\n",
      "        -0.0750,  0.0491,  0.0329, -0.0484,  0.0718, -0.0313, -0.0517, -0.0792,\n",
      "        -0.0481, -0.0021,  0.0170, -0.0225, -0.0163,  0.0489, -0.0215,  0.0324,\n",
      "         0.0123, -0.0713,  0.0787, -0.0472,  0.0577, -0.0441, -0.0594,  0.0646,\n",
      "        -0.0723, -0.0387, -0.0598,  0.0559, -0.0700, -0.0828,  0.0208,  0.0462,\n",
      "         0.0753,  0.0033, -0.0062,  0.0873, -0.0443, -0.0646,  0.0363,  0.0243,\n",
      "        -0.0588,  0.0340, -0.0668, -0.0754, -0.0355, -0.0793,  0.0116,  0.0005,\n",
      "         0.0554, -0.0612, -0.0084,  0.0493, -0.0087,  0.0841,  0.0161,  0.0307,\n",
      "         0.0068, -0.0320, -0.0317,  0.0247,  0.0576,  0.0726, -0.0280,  0.0884,\n",
      "        -0.0303,  0.0036,  0.0606,  0.0013, -0.0477,  0.0628,  0.0007,  0.0220,\n",
      "         0.0733, -0.0486,  0.0480, -0.0384, -0.0686,  0.0537,  0.0091, -0.0803,\n",
      "         0.0365, -0.0074, -0.0020, -0.0084, -0.0101, -0.0732, -0.0739, -0.0508,\n",
      "        -0.0655, -0.0645, -0.0777, -0.0421,  0.0573,  0.0513,  0.0421,  0.0138,\n",
      "         0.0416, -0.0190, -0.0132, -0.0354, -0.0779,  0.0669,  0.0015, -0.0135,\n",
      "        -0.0175, -0.0424,  0.0027,  0.0572,  0.0321,  0.0587, -0.0614, -0.0487,\n",
      "        -0.0381, -0.0516, -0.0767, -0.0867,  0.0519,  0.0714,  0.0467,  0.0513,\n",
      "         0.0545,  0.0476,  0.0236,  0.0313, -0.0733, -0.0501,  0.0040,  0.0364,\n",
      "         0.0672, -0.0847, -0.0114, -0.0332,  0.0654, -0.0407, -0.0574,  0.0729,\n",
      "         0.0310,  0.0083, -0.0152, -0.0637, -0.0242,  0.0071,  0.0524, -0.0349,\n",
      "         0.0791, -0.0078,  0.0491,  0.0011, -0.0551,  0.0411,  0.0499, -0.0632,\n",
      "        -0.0379,  0.0817,  0.0407, -0.0455, -0.0010, -0.0292, -0.0837,  0.0841,\n",
      "        -0.0273, -0.0334,  0.0352, -0.0719, -0.0378,  0.0477,  0.0089, -0.0837,\n",
      "        -0.0346,  0.0610,  0.0280,  0.0146,  0.0262,  0.0181, -0.0595,  0.0863,\n",
      "         0.0364, -0.0211,  0.0208, -0.0436,  0.0007,  0.0334, -0.0426,  0.0011,\n",
      "        -0.0704,  0.0514, -0.0232, -0.0033, -0.0870, -0.0497,  0.0613, -0.0489,\n",
      "         0.0070, -0.0300,  0.0782, -0.0072, -0.0484, -0.0110,  0.0175,  0.0681,\n",
      "        -0.0243, -0.0099, -0.0417, -0.0365, -0.0808, -0.0520, -0.0270, -0.0781,\n",
      "         0.0196,  0.0529, -0.0253, -0.0309, -0.0765,  0.0394,  0.0768,  0.0298,\n",
      "         0.0298,  0.0612,  0.0131,  0.0674, -0.0445,  0.0859,  0.0211, -0.0139,\n",
      "        -0.0502, -0.0284, -0.0430,  0.0137,  0.0466, -0.0055,  0.0571, -0.0449,\n",
      "        -0.0153, -0.0326, -0.0143, -0.0878,  0.0295, -0.0868,  0.0645, -0.0050,\n",
      "        -0.0088,  0.0464, -0.0076,  0.0846,  0.0463,  0.0477, -0.0654, -0.0449,\n",
      "        -0.0807,  0.0779,  0.0026,  0.0249,  0.0108, -0.0465,  0.0789, -0.0646,\n",
      "         0.0035, -0.0444, -0.0190, -0.0506,  0.0883, -0.0271,  0.0854,  0.0522,\n",
      "         0.0829, -0.0789, -0.0767, -0.0016,  0.0522,  0.0612, -0.0170,  0.0046,\n",
      "         0.0724,  0.0031, -0.0700, -0.0426,  0.0397, -0.0256,  0.0150, -0.0383,\n",
      "        -0.0878,  0.0332, -0.0869,  0.0556,  0.0061, -0.0614,  0.0212,  0.0070,\n",
      "        -0.0637, -0.0711,  0.0361,  0.0204,  0.0547,  0.0066, -0.0103,  0.0688,\n",
      "         0.0724,  0.0139, -0.0634,  0.0275,  0.0601, -0.0561, -0.0220, -0.0168])), ('lstm2.weight_ih_l0', tensor([[ 0.0506, -0.0346,  0.0083,  ..., -0.0026, -0.0375, -0.0273],\n",
      "        [-0.0457, -0.0460,  0.0050,  ...,  0.0555,  0.0476, -0.0005],\n",
      "        [ 0.0048,  0.0332,  0.0444,  ..., -0.0057, -0.0275,  0.0437],\n",
      "        ...,\n",
      "        [ 0.0364,  0.0456,  0.0406,  ..., -0.0095,  0.0357,  0.0600],\n",
      "        [ 0.0519,  0.0305, -0.0226,  ..., -0.0379,  0.0335,  0.0416],\n",
      "        [ 0.0572,  0.0136,  0.0226,  ..., -0.0236, -0.0320,  0.0059]])), ('lstm2.weight_hh_l0', tensor([[-0.0407, -0.0177, -0.0074,  ...,  0.0135, -0.0146,  0.0037],\n",
      "        [ 0.0404, -0.0331, -0.0475,  ..., -0.0329, -0.0259, -0.0376],\n",
      "        [ 0.0067, -0.0357,  0.0608,  ..., -0.0309,  0.0617,  0.0462],\n",
      "        ...,\n",
      "        [ 0.0114, -0.0520, -0.0312,  ...,  0.0596, -0.0048,  0.0085],\n",
      "        [-0.0513,  0.0332,  0.0461,  ...,  0.0073,  0.0338,  0.0063],\n",
      "        [-0.0236,  0.0019, -0.0426,  ..., -0.0535,  0.0421,  0.0346]])), ('lstm2.bias_ih_l0', tensor([-0.0388, -0.0319, -0.0331,  ...,  0.0199, -0.0471,  0.0084])), ('lstm2.bias_hh_l0', tensor([ 0.0382,  0.0548,  0.0523,  ...,  0.0528, -0.0007,  0.0022])), ('lstm3.weight_ih_l0', tensor([[-0.0111,  0.0022, -0.0019,  ..., -0.0061,  0.0013, -0.0256],\n",
      "        [-0.0239,  0.0101, -0.0047,  ..., -0.0339,  0.0299,  0.0113],\n",
      "        [ 0.0256, -0.0254,  0.0154,  ..., -0.0179,  0.0429, -0.0410],\n",
      "        ...,\n",
      "        [-0.0211, -0.0123,  0.0096,  ..., -0.0435, -0.0227, -0.0264],\n",
      "        [ 0.0035,  0.0300,  0.0269,  ..., -0.0231, -0.0401,  0.0213],\n",
      "        [-0.0261, -0.0020, -0.0015,  ..., -0.0018, -0.0344, -0.0422]])), ('lstm3.weight_hh_l0', tensor([[ 0.0181,  0.0097,  0.0210,  ..., -0.0441, -0.0323,  0.0035],\n",
      "        [ 0.0068,  0.0199,  0.0307,  ..., -0.0291, -0.0247,  0.0006],\n",
      "        [ 0.0197,  0.0086,  0.0083,  ...,  0.0291, -0.0019,  0.0383],\n",
      "        ...,\n",
      "        [ 0.0237, -0.0010, -0.0179,  ..., -0.0034, -0.0176,  0.0359],\n",
      "        [-0.0113,  0.0211,  0.0116,  ..., -0.0302, -0.0231, -0.0121],\n",
      "        [ 0.0120,  0.0166,  0.0371,  ...,  0.0265, -0.0079,  0.0337]])), ('lstm3.bias_ih_l0', tensor([ 0.0192, -0.0407,  0.0370,  ...,  0.0318, -0.0151,  0.0244])), ('lstm3.bias_hh_l0', tensor([-0.0128,  0.0228,  0.0361,  ..., -0.0208,  0.0404, -0.0173])), ('lstm4.weight_ih_l0', tensor([[-0.0375,  0.0435,  0.0312,  ...,  0.0304,  0.0120, -0.0081],\n",
      "        [ 0.0601,  0.0487, -0.0558,  ...,  0.0032, -0.0222,  0.0543],\n",
      "        [-0.0162, -0.0367,  0.0150,  ...,  0.0373, -0.0587, -0.0520],\n",
      "        ...,\n",
      "        [-0.0141, -0.0123,  0.0286,  ..., -0.0150,  0.0365, -0.0138],\n",
      "        [ 0.0263, -0.0575, -0.0535,  ...,  0.0018, -0.0478,  0.0185],\n",
      "        [-0.0024,  0.0612,  0.0541,  ..., -0.0066, -0.0355, -0.0577]])), ('lstm4.weight_hh_l0', tensor([[-0.0583, -0.0566, -0.0378,  ..., -0.0314, -0.0073, -0.0437],\n",
      "        [-0.0367,  0.0427,  0.0242,  ..., -0.0506, -0.0114, -0.0297],\n",
      "        [-0.0580,  0.0025, -0.0325,  ..., -0.0498,  0.0546,  0.0530],\n",
      "        ...,\n",
      "        [ 0.0306, -0.0291, -0.0406,  ..., -0.0144, -0.0301, -0.0419],\n",
      "        [-0.0561, -0.0117,  0.0304,  ..., -0.0080,  0.0619, -0.0288],\n",
      "        [ 0.0451,  0.0486,  0.0116,  ..., -0.0622, -0.0039,  0.0035]])), ('lstm4.bias_ih_l0', tensor([ 0.0322, -0.0167, -0.0278,  ...,  0.0395, -0.0593,  0.0543])), ('lstm4.bias_hh_l0', tensor([-0.0020,  0.0195, -0.0286,  ...,  0.0124,  0.0291,  0.0422])), ('lstm5.weight_ih_l0', tensor([[ 0.0772,  0.0388, -0.0227,  ...,  0.0587,  0.0386, -0.0274],\n",
      "        [-0.0663, -0.0167,  0.0184,  ..., -0.0521, -0.0406, -0.0676],\n",
      "        [-0.0210,  0.0463,  0.0741,  ..., -0.0339,  0.0579, -0.0723],\n",
      "        ...,\n",
      "        [ 0.0745, -0.0629, -0.0183,  ...,  0.0189, -0.0763, -0.0293],\n",
      "        [ 0.0099, -0.0383,  0.0002,  ..., -0.0256,  0.0484,  0.0784],\n",
      "        [-0.0373,  0.0344, -0.0086,  ..., -0.0114, -0.0786, -0.0339]])), ('lstm5.weight_hh_l0', tensor([[-0.0858, -0.0862,  0.0681,  ..., -0.0365,  0.0645,  0.0525],\n",
      "        [-0.0254,  0.0736,  0.0442,  ..., -0.0115, -0.0558,  0.0425],\n",
      "        [ 0.0781, -0.0252, -0.0187,  ..., -0.0841, -0.0796,  0.0367],\n",
      "        ...,\n",
      "        [ 0.0576,  0.0803,  0.0305,  ..., -0.0375, -0.0149,  0.0799],\n",
      "        [ 0.0207,  0.0779,  0.0862,  ..., -0.0578,  0.0252,  0.0269],\n",
      "        [-0.0678,  0.0310, -0.0599,  ..., -0.0208,  0.0162,  0.0856]])), ('lstm5.bias_ih_l0', tensor([ 0.0685,  0.0805, -0.0317, -0.0142, -0.0268, -0.0264,  0.0564,  0.0182,\n",
      "        -0.0574,  0.0232,  0.0049, -0.0103, -0.0208, -0.0647, -0.0196, -0.0854,\n",
      "         0.0748,  0.0549,  0.0377,  0.0523,  0.0168, -0.0716, -0.0425, -0.0374,\n",
      "        -0.0435,  0.0438,  0.0757, -0.0118,  0.0426,  0.0159,  0.0902, -0.0040,\n",
      "        -0.0469,  0.0367, -0.0652,  0.0170,  0.0586, -0.0501, -0.0525,  0.0353,\n",
      "        -0.0475,  0.0017,  0.0531,  0.0218,  0.0383, -0.0505,  0.0454, -0.0392,\n",
      "         0.0442,  0.0501,  0.0375,  0.0009, -0.0049, -0.0072, -0.0404,  0.0865,\n",
      "         0.0182,  0.0362, -0.0570, -0.0763,  0.0708, -0.0746, -0.0543, -0.0573,\n",
      "        -0.0416, -0.0391, -0.0713, -0.0212, -0.0854, -0.0755, -0.0069,  0.0859,\n",
      "        -0.0576,  0.0826,  0.0250, -0.0227, -0.0633,  0.0834, -0.0328,  0.0806,\n",
      "        -0.0162,  0.0650, -0.0242,  0.0772,  0.0824, -0.0247,  0.0826,  0.0857,\n",
      "        -0.0440,  0.0650, -0.0201, -0.0313, -0.0032,  0.0537, -0.0306,  0.0872,\n",
      "        -0.0631,  0.0063,  0.0649, -0.0756, -0.0557, -0.0626,  0.0769, -0.0206,\n",
      "        -0.0379, -0.0496, -0.0032,  0.0097, -0.0652, -0.0542, -0.0469, -0.0180,\n",
      "         0.0226,  0.0791, -0.0603,  0.0680,  0.0534, -0.0522,  0.0613,  0.0762,\n",
      "        -0.0252, -0.0791, -0.0467, -0.0229, -0.0792, -0.0284, -0.0601, -0.0157,\n",
      "        -0.0863, -0.0158,  0.0336,  0.0118, -0.0710, -0.0692,  0.0603, -0.0685,\n",
      "         0.0348,  0.0366,  0.0630,  0.0847, -0.0249,  0.0306,  0.0317, -0.0761,\n",
      "         0.0660,  0.0396, -0.0591,  0.0701,  0.0756,  0.0421, -0.0208, -0.0855,\n",
      "         0.0033, -0.0815, -0.0246, -0.0706,  0.0191, -0.0300, -0.0770, -0.0377,\n",
      "        -0.0465,  0.0283,  0.0629,  0.0484,  0.0431,  0.0649, -0.0284, -0.0584,\n",
      "         0.0250, -0.0461, -0.0330, -0.0450,  0.0356,  0.0464, -0.0432,  0.0846,\n",
      "         0.0298, -0.0346, -0.0440,  0.0585,  0.0663,  0.0301,  0.0834,  0.0377,\n",
      "        -0.0007,  0.0145, -0.0232, -0.0130, -0.0701, -0.0681, -0.0701,  0.0263,\n",
      "        -0.0401, -0.0200, -0.0567, -0.0110,  0.0007, -0.0866, -0.0300, -0.0531,\n",
      "         0.0051,  0.0798,  0.0339, -0.0874,  0.0051, -0.0589,  0.0353, -0.0043,\n",
      "        -0.0283, -0.0344, -0.0430,  0.0319,  0.0109,  0.0479, -0.0533,  0.0690,\n",
      "         0.0764, -0.0005, -0.0424,  0.0092,  0.0434,  0.0670,  0.0601, -0.0441,\n",
      "         0.0175,  0.0550,  0.0013, -0.0861, -0.0199, -0.0503,  0.0736,  0.0643,\n",
      "         0.0780,  0.0621, -0.0034, -0.0129,  0.0509,  0.0511,  0.0212,  0.0861,\n",
      "         0.0594,  0.0457, -0.0272, -0.0260, -0.0666,  0.0797,  0.0113, -0.0047,\n",
      "        -0.0067,  0.0110, -0.0792,  0.0067,  0.0185,  0.0621, -0.0521, -0.0741,\n",
      "        -0.0558, -0.0461, -0.0213,  0.0450, -0.0493,  0.0554, -0.0527, -0.0376,\n",
      "         0.0209, -0.0444,  0.0736,  0.0311, -0.0662,  0.0329,  0.0224, -0.0073,\n",
      "         0.0250,  0.0184,  0.0600,  0.0511, -0.0016,  0.0051,  0.0173,  0.0280,\n",
      "         0.0768, -0.0576,  0.0785,  0.0105, -0.0765, -0.0091, -0.0437, -0.0240,\n",
      "        -0.0859, -0.0835,  0.0006,  0.0775,  0.0379, -0.0331, -0.0488,  0.0565,\n",
      "         0.0189, -0.0168,  0.0220,  0.0627,  0.0409, -0.0503, -0.0683, -0.0214,\n",
      "        -0.0441, -0.0849,  0.0999, -0.0015, -0.0721,  0.0203,  0.0228,  0.1088,\n",
      "        -0.0137, -0.0264, -0.0054,  0.0377,  0.0069,  0.0093,  0.0890,  0.0113,\n",
      "         0.0481,  0.0145, -0.0523, -0.0506, -0.0401, -0.0779,  0.0548,  0.0784,\n",
      "        -0.0503, -0.0104, -0.0396,  0.0496, -0.0829, -0.0554,  0.0052,  0.0898,\n",
      "         0.0502,  0.0260, -0.0925, -0.0520, -0.0613,  0.0206,  0.0412, -0.0501,\n",
      "         0.0178,  0.0533,  0.0522, -0.0576, -0.0923, -0.0758, -0.0185,  0.0537,\n",
      "         0.0377,  0.0010, -0.0644,  0.0251,  0.0627,  0.0158,  0.0683, -0.0743,\n",
      "        -0.0626, -0.0605,  0.0443,  0.0565, -0.0348, -0.0032,  0.0299, -0.1061,\n",
      "         0.0213, -0.0483, -0.0224,  0.0568,  0.0192, -0.0402, -0.0373,  0.0721,\n",
      "         0.0272,  0.0495,  0.0398, -0.0709, -0.0723,  0.0638, -0.0494, -0.0246,\n",
      "        -0.0812, -0.0478,  0.0793, -0.0175,  0.0738,  0.0797,  0.0787, -0.0341,\n",
      "         0.0757,  0.0304,  0.0448, -0.0347,  0.0602, -0.0823, -0.0148, -0.0879,\n",
      "        -0.0741, -0.0145,  0.0595, -0.0587,  0.0512,  0.0066,  0.0390, -0.0403,\n",
      "         0.0138, -0.0237,  0.0738,  0.0753,  0.0829,  0.0519, -0.0262,  0.0724,\n",
      "        -0.0835, -0.0284,  0.0753,  0.0291,  0.0711, -0.0069,  0.0686, -0.0451,\n",
      "        -0.0142,  0.0773, -0.0392, -0.0638,  0.0709,  0.0099,  0.0248, -0.0368,\n",
      "         0.0014, -0.0297, -0.0540,  0.0308, -0.0665, -0.0833, -0.0254,  0.0830,\n",
      "        -0.0597, -0.0674,  0.0635,  0.0157, -0.0144,  0.0803,  0.0691, -0.0210,\n",
      "         0.0728, -0.0097,  0.0264,  0.0386,  0.0639, -0.0232, -0.0333, -0.0695,\n",
      "        -0.0744,  0.0308, -0.0818, -0.0414, -0.0074, -0.0194, -0.0773,  0.0109,\n",
      "        -0.0248, -0.0039, -0.0388, -0.0141, -0.0725, -0.0054, -0.0776,  0.0610,\n",
      "         0.0753, -0.0796, -0.0235, -0.0041,  0.0338,  0.0148,  0.0292, -0.0703,\n",
      "        -0.0818,  0.0348,  0.0209,  0.0407, -0.0104,  0.0368, -0.0536,  0.0539,\n",
      "         0.0566,  0.0285,  0.0029,  0.0342, -0.0859, -0.0784,  0.0831, -0.0639,\n",
      "        -0.0597, -0.0508, -0.0785,  0.0659,  0.0021,  0.0635,  0.0505,  0.0775,\n",
      "         0.0208,  0.0166,  0.0162, -0.0722,  0.0486,  0.0526, -0.0257,  0.0669])), ('lstm5.bias_hh_l0', tensor([-0.0849, -0.0635, -0.0137, -0.0188, -0.0589, -0.0661, -0.0353, -0.0078,\n",
      "        -0.0245, -0.0250,  0.0435,  0.0708,  0.0270, -0.0781, -0.0225,  0.0490,\n",
      "         0.0854, -0.0711,  0.0883, -0.0116, -0.0590,  0.0179,  0.0199,  0.0499,\n",
      "        -0.0638,  0.0363, -0.0437, -0.0269,  0.0087,  0.0496, -0.0246,  0.0315,\n",
      "        -0.0616, -0.0642, -0.0647,  0.0494, -0.0710,  0.0179, -0.0110, -0.0535,\n",
      "        -0.0246, -0.0456, -0.0698, -0.0495, -0.0307, -0.0827, -0.0655, -0.0543,\n",
      "        -0.0311, -0.0312,  0.0399,  0.0145,  0.0142,  0.0378,  0.0821, -0.0055,\n",
      "        -0.0703, -0.0189,  0.0785,  0.0408, -0.0087,  0.0195, -0.0632,  0.0401,\n",
      "        -0.0367,  0.0555,  0.0802,  0.0651, -0.0811,  0.0669, -0.0469, -0.0611,\n",
      "        -0.0320,  0.0221, -0.0729, -0.0183, -0.0416, -0.0255, -0.0813,  0.0878,\n",
      "         0.0789,  0.0037,  0.0248, -0.0200, -0.0870,  0.0638,  0.0564,  0.0275,\n",
      "         0.0375,  0.0072, -0.0403, -0.0498,  0.0823,  0.0229,  0.0101, -0.0177,\n",
      "         0.0204, -0.0342, -0.0283,  0.0150,  0.0311,  0.0553, -0.0204, -0.0414,\n",
      "        -0.0385, -0.0263, -0.0186, -0.0520,  0.0630, -0.0178,  0.0071,  0.0666,\n",
      "        -0.0294, -0.0263,  0.0332,  0.0684, -0.0224,  0.0529,  0.0126,  0.0424,\n",
      "        -0.0231, -0.0528,  0.0196, -0.0647,  0.0756,  0.0500,  0.0351,  0.0216,\n",
      "         0.0296,  0.0086,  0.0107,  0.0385, -0.0383,  0.0359,  0.0481,  0.0872,\n",
      "        -0.0116,  0.0128,  0.0711, -0.0683, -0.0721,  0.0866,  0.0434,  0.0608,\n",
      "        -0.0719,  0.0599, -0.0224, -0.0686,  0.0652,  0.0512,  0.0066,  0.0799,\n",
      "         0.0105,  0.0463,  0.0363, -0.0121,  0.0660,  0.0441,  0.0146,  0.0490,\n",
      "         0.0161, -0.0047,  0.0085, -0.0429,  0.0769,  0.0472, -0.0540,  0.0551,\n",
      "        -0.0438,  0.0682,  0.0641, -0.0254, -0.0881,  0.0794, -0.0808,  0.0714,\n",
      "        -0.0070,  0.0868,  0.0386,  0.0010, -0.0534,  0.0606,  0.0520, -0.0687,\n",
      "         0.0449,  0.0697, -0.0068,  0.0019, -0.0878, -0.0083,  0.0826, -0.0023,\n",
      "         0.0481,  0.0739, -0.0441,  0.0442, -0.0273, -0.0424, -0.0248, -0.0425,\n",
      "         0.0370, -0.0596,  0.0789, -0.0385,  0.0652,  0.0357, -0.0012,  0.0811,\n",
      "         0.0392,  0.0770,  0.0446, -0.0774,  0.0349, -0.0250,  0.0848, -0.0406,\n",
      "         0.0388, -0.0322,  0.0782,  0.0478,  0.0716,  0.0396,  0.0360,  0.0091,\n",
      "        -0.0244, -0.0627,  0.0684,  0.0142, -0.0811, -0.0148, -0.0851, -0.0361,\n",
      "        -0.0204,  0.0522,  0.0687, -0.0129, -0.0657,  0.0462,  0.0375,  0.0798,\n",
      "        -0.0356, -0.0078, -0.0502,  0.0393,  0.0554, -0.0825, -0.0004, -0.0329,\n",
      "        -0.0159, -0.0284,  0.0811,  0.0157,  0.0356,  0.0053, -0.0792, -0.0180,\n",
      "         0.0301,  0.0575,  0.0052, -0.0795,  0.0132,  0.0346, -0.0351,  0.0142,\n",
      "         0.0419,  0.0381,  0.0445, -0.0877,  0.0903,  0.0611, -0.0241, -0.0091,\n",
      "        -0.0023, -0.0648, -0.0050, -0.0386,  0.0469,  0.0696,  0.0043,  0.0517,\n",
      "         0.0150,  0.0700,  0.0658, -0.0874,  0.0336, -0.0818, -0.0554, -0.0348,\n",
      "         0.0032, -0.0912,  0.0937,  0.0354, -0.0453, -0.0311,  0.0748,  0.0011,\n",
      "         0.0054,  0.0205,  0.0461,  0.0297,  0.0019,  0.0831,  0.0314,  0.0245,\n",
      "        -0.0847,  0.0216,  0.0221,  0.0645, -0.0596,  0.0433,  0.0206,  0.0713,\n",
      "         0.0665,  0.0696, -0.0009, -0.0117,  0.0072,  0.0687,  0.0294,  0.0315,\n",
      "        -0.0521, -0.0419, -0.0045,  0.0072, -0.0593,  0.0131, -0.0334, -0.0461,\n",
      "        -0.0074,  0.0767, -0.0621,  0.0596, -0.0398,  0.0304,  0.0333, -0.0660,\n",
      "        -0.0324, -0.0546, -0.0573, -0.0383,  0.0247,  0.0393, -0.0475,  0.0978,\n",
      "         0.1022,  0.0135,  0.0808,  0.0346, -0.0336,  0.0421,  0.0392,  0.0729,\n",
      "         0.0031,  0.0317,  0.0640, -0.0206, -0.0912,  0.0862,  0.0071, -0.0240,\n",
      "        -0.0255,  0.0728,  0.0705, -0.0277, -0.0207, -0.0442, -0.0383, -0.0687,\n",
      "        -0.0416, -0.0221, -0.0056, -0.0583,  0.0963, -0.0646, -0.0709, -0.0357,\n",
      "        -0.0186,  0.0029, -0.0716,  0.0791, -0.0301,  0.0151, -0.0584,  0.0890,\n",
      "        -0.0356, -0.0620,  0.0838, -0.0876, -0.0528,  0.0096, -0.0589,  0.0760,\n",
      "         0.0686, -0.0699, -0.0061,  0.0414, -0.0628, -0.0665, -0.0230, -0.0309,\n",
      "        -0.0038, -0.0163,  0.0692,  0.0197,  0.0409, -0.0373,  0.0395,  0.0130,\n",
      "         0.0394, -0.0553,  0.0109,  0.0846, -0.0821, -0.0043, -0.0230, -0.0612,\n",
      "         0.0748,  0.0200, -0.0769,  0.0112, -0.0217,  0.0692, -0.0161, -0.0385,\n",
      "         0.0878, -0.0127,  0.0569, -0.0026, -0.0227,  0.0325,  0.0225, -0.0807,\n",
      "         0.0026, -0.0880, -0.0665, -0.0164,  0.0461,  0.0005, -0.0782, -0.0263,\n",
      "         0.0380, -0.0108, -0.0746, -0.0606, -0.0495,  0.0856,  0.0215, -0.0652,\n",
      "         0.0154, -0.0492, -0.0081, -0.0501,  0.0501,  0.0433,  0.0567, -0.0550,\n",
      "        -0.0737, -0.0323, -0.0202, -0.0491, -0.0099,  0.0843,  0.0081, -0.0806,\n",
      "        -0.0726, -0.0168, -0.0824, -0.0341,  0.0660, -0.0776,  0.0003,  0.0340,\n",
      "         0.0629, -0.0216,  0.0660, -0.0593,  0.0788, -0.0848, -0.0366, -0.0540,\n",
      "         0.0264,  0.0269, -0.0788, -0.0766, -0.0347,  0.0693,  0.0550, -0.0663,\n",
      "         0.0288,  0.0614, -0.0595,  0.0729,  0.0386,  0.0019, -0.0336,  0.0596,\n",
      "        -0.0609, -0.0153,  0.0275,  0.0269, -0.0472,  0.0031, -0.0447,  0.0476,\n",
      "         0.0779, -0.0852, -0.0515,  0.0342, -0.0740, -0.0601,  0.0327, -0.0089])), ('lstm6.weight_ih_l0', tensor([[ 0.0403, -0.0218, -0.0883,  ...,  0.0301,  0.0519,  0.0589],\n",
      "        [ 0.0376,  0.0242, -0.0281,  ..., -0.0672,  0.0395,  0.0262],\n",
      "        [-0.1013,  0.0586,  0.1038,  ...,  0.0455, -0.0900, -0.0098],\n",
      "        ...,\n",
      "        [-0.1198, -0.0736, -0.0147,  ..., -0.0378,  0.0588,  0.1152],\n",
      "        [-0.0659, -0.0190, -0.0510,  ..., -0.0565,  0.0948,  0.0102],\n",
      "        [-0.0643,  0.1238, -0.0814,  ..., -0.0789,  0.0127, -0.0818]])), ('lstm6.weight_hh_l0', tensor([[-0.0660, -0.0036,  0.0452,  ...,  0.0144,  0.1195,  0.0615],\n",
      "        [ 0.0580, -0.0979,  0.0650,  ..., -0.0320,  0.0202, -0.0460],\n",
      "        [ 0.0545, -0.0812, -0.0042,  ..., -0.1045,  0.0691,  0.0438],\n",
      "        ...,\n",
      "        [ 0.0796, -0.0083, -0.1197,  ...,  0.0082,  0.0588, -0.0281],\n",
      "        [-0.1066, -0.0148, -0.0196,  ...,  0.0281,  0.0730,  0.0310],\n",
      "        [-0.0042, -0.0703,  0.0542,  ...,  0.0205, -0.0774, -0.0962]])), ('lstm6.bias_ih_l0', tensor([ 0.0381, -0.0809,  0.0390,  0.0744, -0.0036,  0.0942,  0.0270,  0.0256,\n",
      "        -0.0217,  0.1185, -0.0544,  0.0197, -0.0861, -0.0376,  0.0019,  0.0800,\n",
      "        -0.0035,  0.0265,  0.0083, -0.0607,  0.1111, -0.1037,  0.0603,  0.1212,\n",
      "         0.0286,  0.0738,  0.0457, -0.0555, -0.0827,  0.1236,  0.0146,  0.0939,\n",
      "         0.0757,  0.0725, -0.1012, -0.1155, -0.0250,  0.0231,  0.1301,  0.0557,\n",
      "         0.1053,  0.0306, -0.0023, -0.0027,  0.0785,  0.1137, -0.1171,  0.0942,\n",
      "        -0.0296,  0.0215, -0.0844, -0.0037,  0.0345, -0.0527, -0.0367,  0.0405,\n",
      "        -0.1222,  0.0028,  0.0177,  0.0291,  0.0958,  0.0092,  0.0043, -0.1019,\n",
      "        -0.0040,  0.0837, -0.0033,  0.0011, -0.0995, -0.0936,  0.0538,  0.0418,\n",
      "         0.0936, -0.0180,  0.0274, -0.0983, -0.0168, -0.0607, -0.0861,  0.0610,\n",
      "        -0.0812, -0.0011, -0.1126, -0.0444,  0.1214,  0.0315,  0.0628, -0.0516,\n",
      "        -0.1186, -0.0298,  0.0296, -0.1093,  0.1180,  0.0667,  0.0379,  0.0640,\n",
      "         0.0083,  0.1065,  0.1197,  0.0563, -0.0192,  0.0899,  0.0264,  0.1161,\n",
      "         0.0888, -0.0479,  0.0305,  0.0413, -0.0921, -0.0365, -0.1088,  0.0331,\n",
      "        -0.0723,  0.1142,  0.0876,  0.0412,  0.0140,  0.0215, -0.1103,  0.0670,\n",
      "         0.0934, -0.0777, -0.0399,  0.0650,  0.0642,  0.0471,  0.0404, -0.0020,\n",
      "        -0.0872,  0.0249, -0.0767,  0.0694,  0.0084, -0.0889,  0.0903, -0.0447,\n",
      "        -0.0146, -0.0842,  0.0339, -0.0033, -0.0541, -0.0770,  0.0106, -0.0348,\n",
      "        -0.0232, -0.0837,  0.0736,  0.0318, -0.1126, -0.0961,  0.0201,  0.0309,\n",
      "        -0.0776, -0.1427,  0.0849,  0.0404, -0.1586,  0.1541,  0.1350, -0.1971,\n",
      "        -0.0379, -0.0683,  0.0399, -0.0907,  0.0026, -0.0273,  0.0302, -0.1199,\n",
      "        -0.0515,  0.0037,  0.1462, -0.0478, -0.0252,  0.0083, -0.0733, -0.0757,\n",
      "        -0.0722,  0.0882, -0.0778,  0.0368, -0.0575,  0.1769, -0.0490, -0.1005,\n",
      "        -0.1199, -0.0976,  0.0272,  0.0098,  0.1313, -0.1844, -0.1379, -0.0549,\n",
      "        -0.0569,  0.0446,  0.0330, -0.0764,  0.1248, -0.0699,  0.0135, -0.1074,\n",
      "         0.0712,  0.1134,  0.1008, -0.0503,  0.0800, -0.1180,  0.0324, -0.0900,\n",
      "         0.1119, -0.0265,  0.0468,  0.0896, -0.0698,  0.0961,  0.0335,  0.1122,\n",
      "        -0.0032,  0.0750,  0.1211,  0.0994, -0.1230, -0.0656,  0.1136,  0.1088,\n",
      "        -0.0090, -0.0510,  0.0321, -0.1170,  0.0286,  0.1139,  0.0233, -0.0598,\n",
      "         0.0024, -0.0529,  0.1263, -0.0784, -0.0220,  0.0615,  0.0167,  0.1182,\n",
      "        -0.0007, -0.0971,  0.0021, -0.1183, -0.0944,  0.1232, -0.0073,  0.0371,\n",
      "        -0.0758,  0.0130,  0.0687,  0.1117, -0.0975,  0.1331, -0.0313, -0.1092])), ('lstm6.bias_hh_l0', tensor([-0.1218, -0.0988,  0.1251,  0.0386,  0.0448,  0.0720, -0.0781, -0.1048,\n",
      "        -0.0644, -0.0988,  0.0975,  0.0011, -0.0794, -0.0915,  0.0805, -0.0648,\n",
      "         0.0252,  0.0525, -0.0712, -0.1173,  0.1059,  0.0893,  0.0179, -0.0742,\n",
      "        -0.0682, -0.0090, -0.0328,  0.1106,  0.0567,  0.0035, -0.0892,  0.1043,\n",
      "        -0.1011, -0.1043, -0.0118, -0.1136,  0.0567, -0.0594, -0.0798, -0.0415,\n",
      "        -0.0397,  0.0026,  0.1131, -0.1015, -0.0081, -0.1083, -0.0606, -0.1167,\n",
      "        -0.0942, -0.1009,  0.1154, -0.0819, -0.0519, -0.0936,  0.0090,  0.1098,\n",
      "        -0.0999, -0.0406,  0.1232, -0.1082,  0.0272,  0.0425,  0.0380,  0.1083,\n",
      "         0.0468, -0.0444,  0.1169, -0.0212,  0.0932,  0.1037, -0.0965, -0.0304,\n",
      "         0.0083, -0.0140,  0.0660,  0.1240,  0.1086, -0.0609,  0.0085,  0.1074,\n",
      "         0.1053,  0.0642, -0.0690,  0.0287, -0.0819, -0.0366,  0.0661, -0.0316,\n",
      "         0.0022,  0.1054, -0.0333,  0.0442, -0.0539,  0.0529, -0.0465,  0.1268,\n",
      "        -0.0309,  0.0372, -0.1027, -0.0120, -0.0422, -0.0012,  0.0475, -0.0732,\n",
      "        -0.1246, -0.1165,  0.0917,  0.1085, -0.0997, -0.1125,  0.0931,  0.0963,\n",
      "         0.0960, -0.0596,  0.0305, -0.0568, -0.1132,  0.0928, -0.0847,  0.0622,\n",
      "         0.1240, -0.0657, -0.0367,  0.1215, -0.0524,  0.0523, -0.0204, -0.0931,\n",
      "        -0.0021, -0.0518, -0.0406, -0.0991, -0.0511, -0.0950,  0.0375,  0.1648,\n",
      "         0.0315, -0.1413, -0.0303,  0.1348,  0.0330,  0.0910, -0.1260, -0.0084,\n",
      "        -0.0790,  0.1027, -0.0892,  0.1465, -0.0488,  0.1347,  0.1177, -0.0548,\n",
      "         0.1033,  0.0277, -0.0420, -0.0618,  0.0342,  0.1797,  0.0608, -0.1471,\n",
      "        -0.1718, -0.0511,  0.0901,  0.0595,  0.0180,  0.1119,  0.1805,  0.0980,\n",
      "        -0.0312, -0.0737,  0.0284, -0.0595, -0.1063,  0.1786, -0.1337,  0.0536,\n",
      "         0.0594, -0.0200, -0.0497, -0.0252, -0.1214,  0.1118, -0.1341,  0.0565,\n",
      "         0.0434,  0.0899,  0.0519,  0.1146,  0.0813, -0.1375, -0.1938,  0.0512,\n",
      "        -0.1210,  0.1034, -0.0695, -0.1183,  0.0665,  0.0611, -0.1112,  0.0090,\n",
      "         0.0235, -0.0127,  0.1029, -0.0988, -0.0787,  0.1090,  0.0184,  0.1103,\n",
      "         0.1182, -0.1218, -0.0698, -0.0812,  0.0997,  0.0474, -0.0808,  0.0363,\n",
      "         0.1046, -0.0843, -0.0810,  0.0854, -0.0812,  0.1243, -0.0849, -0.0947,\n",
      "         0.0181,  0.1131,  0.0293, -0.1038, -0.0761, -0.0011, -0.0591,  0.0940,\n",
      "        -0.0168, -0.0851,  0.0334,  0.0196,  0.0020,  0.0817,  0.1032,  0.1076,\n",
      "        -0.0152, -0.0617,  0.0349, -0.1000,  0.0758, -0.0789, -0.0901, -0.0528,\n",
      "         0.0476, -0.0843, -0.0151, -0.0003,  0.1082,  0.0932,  0.0864,  0.0568])), ('lstm7.weight_ih_l0', tensor([[-0.1526,  0.1315, -0.1646,  ..., -0.0143,  0.0690,  0.1284],\n",
      "        [ 0.0337, -0.0101,  0.1489,  ...,  0.0367,  0.0305,  0.0285],\n",
      "        [ 0.0606,  0.0959,  0.0526,  ..., -0.1759,  0.0418, -0.0317],\n",
      "        ...,\n",
      "        [ 0.0441, -0.1056, -0.0015,  ..., -0.1102, -0.0346, -0.1535],\n",
      "        [-0.0567,  0.0578, -0.0490,  ...,  0.0747, -0.0624,  0.0969],\n",
      "        [ 0.1395,  0.1620, -0.0415,  ...,  0.1252, -0.0037,  0.1396]])), ('lstm7.weight_hh_l0', tensor([[ 0.0710, -0.1014,  0.1319,  ...,  0.0706, -0.1096, -0.1555],\n",
      "        [-0.0760, -0.0221, -0.0439,  ...,  0.0492,  0.0902, -0.0270],\n",
      "        [ 0.0441,  0.1594, -0.0546,  ..., -0.1547, -0.0897,  0.0655],\n",
      "        ...,\n",
      "        [-0.1188,  0.0687, -0.0771,  ...,  0.1240,  0.1678, -0.0550],\n",
      "        [ 0.0596, -0.0522,  0.1002,  ...,  0.0980,  0.1710,  0.1506],\n",
      "        [ 0.0349,  0.1595,  0.0078,  ...,  0.0393,  0.0458, -0.1251]])), ('lstm7.bias_ih_l0', tensor([-0.0445,  0.0622,  0.2371,  0.0108,  0.1335, -0.1224, -0.0408, -0.0401,\n",
      "        -0.0770,  0.0931, -0.0379, -0.0773,  0.1645, -0.0270,  0.1101, -0.0046,\n",
      "         0.1354, -0.0556,  0.0777,  0.0639,  0.0710,  0.0987,  0.1692, -0.1044,\n",
      "         0.1377, -0.0496, -0.1331, -0.1065, -0.0585,  0.1059, -0.1182, -0.0298,\n",
      "        -0.1657,  0.0033,  0.2664,  0.0904, -0.0483, -0.1183, -0.0325,  0.1479,\n",
      "        -0.0345, -0.0176,  0.0546, -0.0675, -0.1671, -0.1557,  0.0058, -0.1696,\n",
      "        -0.1583,  0.2115,  0.1032, -0.0870,  0.0097, -0.0996, -0.0317, -0.0057,\n",
      "         0.1551,  0.1859, -0.0034,  0.0066,  0.0738,  0.1388,  0.1432, -0.0074,\n",
      "        -0.0324, -0.0743,  0.3779, -0.1522, -0.0911, -0.0555, -0.1795, -0.0445,\n",
      "        -0.2416,  0.2084,  0.0945,  0.1644, -0.0740, -0.0756, -0.1875,  0.1022,\n",
      "         0.0481, -0.1958, -0.0603, -0.0155, -0.1199,  0.0619,  0.1766, -0.1931,\n",
      "         0.3040, -0.1308, -0.1301, -0.0133, -0.0263, -0.2623, -0.2159, -0.0922,\n",
      "         0.0682, -0.0873,  0.1496,  0.0059, -0.0964,  0.0456,  0.0796,  0.0991,\n",
      "         0.0162,  0.0925, -0.1447,  0.1863,  0.0328, -0.0867,  0.1239,  0.0009,\n",
      "         0.0499, -0.1091, -0.0168,  0.0985, -0.0606,  0.1142, -0.1356, -0.1346,\n",
      "         0.0362, -0.0126,  0.0535,  0.1773,  0.0554, -0.0025, -0.0260,  0.0024])), ('lstm7.bias_hh_l0', tensor([-3.1534e-02, -1.6759e-02,  8.4093e-02, -5.3991e-02, -1.4374e-02,\n",
      "        -7.3838e-02,  1.5882e-01, -3.5185e-03,  4.8929e-02, -1.3107e-01,\n",
      "        -7.0838e-02,  1.3785e-01, -1.6257e-01, -1.1568e-01,  1.3260e-01,\n",
      "        -1.2789e-01, -2.3483e-02,  1.3201e-01,  1.1744e-01, -1.5726e-01,\n",
      "        -1.3872e-01,  1.1115e-01, -7.6154e-02,  8.1728e-02,  2.3008e-01,\n",
      "         1.8495e-01, -1.5234e-01,  1.5540e-01,  1.1865e-01,  3.9598e-02,\n",
      "        -3.1899e-03, -9.8093e-02, -6.7628e-02, -6.3706e-02,  2.8760e-01,\n",
      "         1.3624e-01,  1.6702e-01,  2.8396e-02,  2.4514e-01,  1.5674e-01,\n",
      "         1.7451e-01,  1.4862e-01, -1.6127e-01, -1.3342e-01, -1.3070e-01,\n",
      "         1.7613e-01, -1.0812e-01,  1.5640e-01, -1.5876e-01,  2.1107e-01,\n",
      "        -9.6188e-02,  6.0616e-02, -1.2932e-01,  7.3256e-02,  5.9622e-02,\n",
      "         8.7461e-02,  2.2246e-01,  5.3435e-02,  4.0008e-02,  9.0972e-02,\n",
      "         1.6864e-02, -1.0193e-01,  1.2388e-01, -2.0967e-02,  9.1898e-02,\n",
      "         8.1087e-02,  1.5871e-01, -1.2941e-01, -6.8827e-02,  1.8305e-01,\n",
      "        -3.9559e-01,  5.1750e-02, -2.4853e-01,  1.9993e-01, -1.2220e-01,\n",
      "         3.5775e-04,  4.6920e-02, -5.5561e-03, -9.2869e-02, -1.2926e-01,\n",
      "        -1.2495e-01, -2.9208e-01,  1.8458e-01, -1.7743e-01, -2.1660e-01,\n",
      "        -2.3837e-01,  1.5078e-01, -1.9515e-01,  3.0400e-01, -1.4945e-01,\n",
      "        -2.1377e-01,  2.3245e-01,  1.4317e-01, -2.6561e-01, -3.1453e-02,\n",
      "        -2.0677e-02,  1.7210e-01, -1.2569e-01,  2.9303e-01,  8.7548e-02,\n",
      "         1.2287e-01, -1.3343e-02,  2.4190e-01, -3.2152e-02,  1.0543e-01,\n",
      "        -1.3565e-01, -8.7114e-02,  1.8608e-01, -1.3732e-01,  1.4872e-02,\n",
      "        -1.0931e-01, -1.1994e-01, -3.8835e-02, -9.6887e-02, -4.2033e-03,\n",
      "        -5.3096e-02,  2.3428e-02, -4.6816e-02,  7.8326e-02,  6.0126e-02,\n",
      "         2.4322e-01,  1.6176e-01, -9.0532e-02,  1.0235e-01, -2.6719e-02,\n",
      "        -1.1570e-01,  9.4751e-02,  1.4349e-02])), ('fc.weight', tensor([[ 1.1136e-01,  3.6077e-02, -3.4286e-01,  3.5127e-02,  3.2186e-02,\n",
      "         -1.9420e-01,  1.1656e-01,  2.0373e-01,  4.0007e-02, -1.0381e-01,\n",
      "         -6.8567e-02, -1.4932e-01,  3.9195e-02,  2.7126e-02,  5.4141e-02,\n",
      "         -1.0941e-01, -1.0713e-01,  4.0459e-02, -6.3897e-02,  1.1212e-02,\n",
      "          2.3728e-01, -1.3569e-02, -1.9959e-01,  2.1582e-01, -1.9899e-01,\n",
      "          2.0754e-01, -2.6369e-02,  6.3184e-02,  1.5606e-01,  1.3973e-02,\n",
      "          1.2450e-01, -1.0050e-01],\n",
      "        [ 1.1123e-01, -1.0390e-01, -2.8039e-01, -7.1205e-02, -4.8441e-02,\n",
      "         -1.2710e-01,  1.9308e-01, -1.2288e-01, -4.4933e-02, -8.6304e-02,\n",
      "          4.6275e-02, -8.7636e-02, -1.7222e-01, -1.0542e-01,  1.5963e-01,\n",
      "          5.4249e-02,  1.8778e-02,  4.6146e-02,  1.5645e-02,  8.5138e-02,\n",
      "          9.3987e-02,  2.3287e-01, -1.5471e-01,  9.9316e-02, -1.0486e-01,\n",
      "          4.0175e-02, -7.2529e-02, -1.4848e-01, -6.2094e-02,  1.7069e-01,\n",
      "         -5.6379e-02,  2.0010e-01],\n",
      "        [ 1.1416e-01,  9.9430e-02, -2.7261e-01,  2.2078e-01, -1.4246e-01,\n",
      "          3.9312e-04,  2.4563e-01, -4.2369e-02, -1.2224e-01, -2.3822e-01,\n",
      "          7.4060e-02,  1.5576e-01, -9.0288e-02, -9.6423e-02,  1.0467e-01,\n",
      "          2.9093e-02,  5.5398e-02,  2.4162e-01, -8.3256e-03, -1.2944e-01,\n",
      "          1.3071e-01, -1.2310e-01, -3.9678e-02,  1.8475e-01, -1.4785e-01,\n",
      "         -9.9054e-02,  2.2424e-02, -9.8672e-02, -1.0818e-01,  2.6279e-03,\n",
      "         -3.2071e-02,  1.9818e-01],\n",
      "        [ 1.2407e-01,  1.6628e-01,  6.7644e-01, -2.1061e-01, -1.4648e-01,\n",
      "          5.8983e-03, -4.9953e-01, -1.1462e-01, -3.2040e-01,  1.8336e-01,\n",
      "         -2.0401e-01,  2.4295e-01, -1.3089e-01, -2.3690e-01, -2.3958e-01,\n",
      "          1.4784e-01, -1.2964e-01, -3.2610e-01,  6.9545e-02, -1.2632e-01,\n",
      "         -1.9083e-01, -3.1286e-01,  3.2104e-01, -3.0146e-01,  5.9541e-01,\n",
      "         -3.6545e-01, -2.6828e-01,  3.2054e-01,  4.5298e-03, -3.8329e-01,\n",
      "         -1.8772e-01, -1.9156e-01],\n",
      "        [-1.4679e-01,  1.3418e-01, -2.2557e-01,  1.6071e-02, -3.7827e-02,\n",
      "         -9.5524e-02,  1.7355e-01, -3.1902e-02, -1.2060e-01,  6.9408e-02,\n",
      "          5.8551e-02, -4.3332e-02,  1.1398e-01,  2.5933e-02, -7.2988e-02,\n",
      "          7.9547e-02,  1.0470e-01,  3.0096e-02, -6.0855e-02,  1.2348e-02,\n",
      "          5.9770e-02, -1.2039e-01, -1.3028e-01, -4.1860e-02, -2.4548e-01,\n",
      "          5.2192e-02,  1.4846e-01,  4.2281e-02,  6.2392e-02, -6.8030e-02,\n",
      "         -7.4519e-02,  1.3374e-02]])), ('fc.bias', tensor([-0.4548, -0.4260, -0.4197,  1.3270, -0.2740]))])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 160 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m lstm\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(weights_path))\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(lstm\u001B[38;5;241m.\u001B[39mstate_dict())\n\u001B[1;32m----> 8\u001B[0m weights \u001B[38;5;241m=\u001B[39m lstm\u001B[38;5;241m.\u001B[39mfc\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     10\u001B[0m bias \u001B[38;5;241m=\u001B[39m lstm\u001B[38;5;241m.\u001B[39mfc\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (\u001B[38;5;28minput\u001B[39m, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a Tensor with 160 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(train_accs, label='Training Accuracy')\n",
    "# plt.plot(valid_accs, label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T09:31:49.277689700Z",
     "start_time": "2023-12-13T09:31:49.238416200Z"
    }
   },
   "id": "52a4803347ff7656"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 85\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (86,) and (85,)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(train_loss) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(train_loss), \u001B[38;5;28mlen\u001B[39m(valid_loss))\n\u001B[1;32m---> 10\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTrain Loss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(epochs, valid_loss, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation Loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     12\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain and Validation Loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\matplotlib\\pyplot.py:3575\u001B[0m, in \u001B[0;36mplot\u001B[1;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3567\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[0;32m   3568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\n\u001B[0;32m   3569\u001B[0m     \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3573\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3574\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Line2D]:\n\u001B[1;32m-> 3575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m gca()\u001B[38;5;241m.\u001B[39mplot(\n\u001B[0;32m   3576\u001B[0m         \u001B[38;5;241m*\u001B[39margs,\n\u001B[0;32m   3577\u001B[0m         scalex\u001B[38;5;241m=\u001B[39mscalex,\n\u001B[0;32m   3578\u001B[0m         scaley\u001B[38;5;241m=\u001B[39mscaley,\n\u001B[0;32m   3579\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data} \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m   3580\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3581\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1721\u001B[0m, in \u001B[0;36mAxes.plot\u001B[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1479\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[0;32m   1480\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1720\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[1;32m-> 1721\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[0;32m   1723\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[1;34m(self, axes, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m    301\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    302\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m--> 303\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\matplotlib\\axes\\_base.py:499\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[0;32m    496\u001B[0m     axes\u001B[38;5;241m.\u001B[39myaxis\u001B[38;5;241m.\u001B[39mupdate_units(y)\n\u001B[0;32m    498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m--> 499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y must have same first dimension, but \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    500\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave shapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y can be no greater than 2D, but have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    503\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (86,) and (85,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 주어진 데이터\n",
    "train_loss = [1.6645072927841773, 1.6160603761672974, 1.5690331825843225, 1.5281135990069463, 1.4931477308273315, 1.4632783302894006, 1.437547967984126, 1.4158186545738807, 1.3972229636632478, 1.381659759924962, 1.3682966690797072, 1.3570995514209454, 1.3477443548349233, 1.3398027007396405, 1.332908048079564, 1.3272519340881934, 1.322229816363408, 1.3183267712593079, 1.3146007565351634, 1.3116262142474835, 1.309016580765064, 1.3065645419634306, 1.3046945287631109, 1.3028130072813768, 1.3014338337458098, 1.299690374961266, 1.298569454596593, 1.2972084375528188, 1.296171206694383, 1.2949942854734569, 1.2943815497251658, 1.293615698814392, 1.292873345888578, 1.2920041175989003, 1.2915958853868337, 1.2907948264708886, 1.2904181021910448, 1.289965244439932, 1.289464024397043, 1.2891363226450407, 1.2888218210293696, 1.2884404796820421, 1.2882084158750682, 1.2879399886498084, 1.2874946089891286, 1.2874214695050166, 1.2871180039185743, 1.2869252195725074, 1.2867666941422682, 1.2865648819850042, 1.2866040559915395, 1.2863171054766729, 1.2863825605465815, 1.2859925719407888, 1.2858293469135578, 1.2857366937857408, 1.2858472237220178, 1.2856693680469806, 1.2851948967346778, 1.2853522392419667, 1.2851463372890766, 1.2851203404940093, 1.2852068084936876, 1.2850808409544139, 1.2850570174363942, 1.2848643935643709, 1.284759636108692, 1.2847790534679706, 1.284824325488164, 1.2847741108674269, 1.2844424018493066, 1.2845483834926898, 1.2846238315105438, 1.2843253772992353, 1.2843242448109846, 1.2842233777046204, 1.2845360132364125, 1.2841830941346974, 1.2845137119293213, 1.2843562914774969, 1.2841233679881463, 1.2842514285674462, 1.284220509804212, 1.2841655543217292, 1.2843667199978461]  # train loss 데이터\n",
    "valid_loss = [4.090862214565277, 3.964098572731018, 3.852714091539383, 3.7569140791893005, 3.6746982038021088, 3.6042402386665344, 3.544006198644638, 3.4926357865333557, 3.448920786380768, 3.4118373692035675, 3.3804314732551575, 3.353868752717972, 3.3314106166362762, 3.3123838007450104, 3.296239912509918, 3.282495230436325, 3.2707569301128387, 3.260680377483368, 3.2520035803318024, 3.2444784939289093, 3.2379291355609894, 3.232205718755722, 3.2271696627140045, 3.222735345363617, 3.2187981605529785, 3.215308129787445, 3.212197184562683, 3.2094317972660065, 3.206957906484604, 3.204738438129425, 3.2027608454227448, 3.200996845960617, 3.1994248926639557, 3.198022186756134, 3.1967703104019165, 3.1956611573696136, 3.1946721971035004, 3.1937944293022156, 3.193017363548279, 3.1923393607139587, 3.1917509138584137, 3.1912316977977753, 3.1907759308815002, 3.19038850069046, 3.190050482749939, 3.189771205186844, 3.189537525177002, 3.189339339733124, 3.1891814172267914, 3.1890613734722137, 3.18897345662117, 3.1889072358608246, 3.1888664066791534, 3.1888466477394104, 3.1888527274131775, 3.1888833343982697, 3.1889193058013916, 3.188969135284424, 3.189032018184662, 3.189104199409485, 3.1891914308071136, 3.189284026622772, 3.1893858313560486, 3.1894978880882263, 3.1896104216575623, 3.189728409051895, 3.1898582577705383, 3.1899873316287994, 3.1901169419288635, 3.190248131752014, 3.1903818249702454, 3.1905190646648407, 3.1906570196151733, 3.1908005475997925, 3.1909464299678802, 3.1910910606384277, 3.191228300333023, 3.191367268562317, 3.1915021538734436, 3.191645920276642, 3.1917780339717865, 3.1919074058532715, 3.1920455992221832, 3.192184418439865, 3.1923215091228485]   # validation loss 데이터\n",
    "\n",
    "\n",
    "# 그래프 그리기\n",
    "epochs = range(len(train_loss) + 1)\n",
    "print(len(train_loss), len(valid_loss))\n",
    "plt.plot(epochs, train_loss, label='Train Loss')\n",
    "plt.plot(epochs, valid_loss, label='Validation Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T05:09:50.585666200Z",
     "start_time": "2023-12-18T05:09:50.497711900Z"
    }
   },
   "id": "e4826ea488105506"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc10efac063048eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
